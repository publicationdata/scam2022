{"type":{"0":"W","1":"W","2":"W","3":"W","4":"W","5":"W","6":"W","7":"W","8":"C","9":"W","10":"C","11":"C","12":"R","13":"C","14":"C","15":"R","16":"W","17":"W","18":"R","19":"R","20":"R","21":"R","22":"R","23":"R","24":"R","25":"R","26":"R","27":"R","28":"R","29":"R","30":"R","31":"R","32":"R","33":"R","34":"R","35":"R"},"module":{"0":"09_01_softmax_loss","1":"12_2_hello_rnn","2":"13_1_rnn_classification_basics","3":"13_2_rnn_classification","4":"13_2_rnn_classification","5":"13_2_rnn_classification","6":"13_3_char_rnn","7":"13_3_char_rnn","8":"13_3_char_rnn","9":"13_3_char_rnn","10":"14_1_seq2seq","11":"14_1_seq2seq","12":"14_1_seq2seq","13":"14_2_seq2seq_att","14":"14_2_seq2seq_att","15":"14_2_seq2seq_att","16":"name_dataset","17":"seq2seq_models","18":"text_loader","19":"text_loader","20":"text_loader","21":"text_loader","22":"text_loader","23":"text_loader","24":"text_loader","25":"text_loader","26":"text_loader","27":"text_loader","28":"text_loader","29":"text_loader","30":"text_loader","31":"text_loader","32":"text_loader","33":"text_loader","34":"text_loader","35":"text_loader"},"obj":{"0":"","1":"","2":"RNNClassifier.forward","3":"RNNClassifier.forward","4":"train","5":"test","6":"RNN.forward","7":"train_teacher_forching","8":"train_teacher_forching","9":"train","10":"test","11":"train","12":"","13":"test","14":"train","15":"","16":"NameDataset.get_country","17":"DecoderRNN.forward","18":"","19":"","20":"","21":"","22":"","23":"","24":"","25":"","26":"","27":"","28":"","29":"","30":"","31":"","32":"","33":"","34":"","35":""},"lnum":{"0":1,"1":73,"2":29,"3":103,"4":140,"5":164,"6":32,"7":90,"8":96,"9":108,"10":26,"11":48,"12":117,"13":40,"14":65,"15":134,"16":37,"17":73,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1},"col":{"0":0,"1":8,"2":22,"3":22,"4":8,"5":8,"6":22,"7":4,"8":4,"9":4,"10":4,"11":4,"12":11,"13":4,"14":4,"15":15,"16":26,"17":22,"18":0,"19":0,"20":0,"21":0,"22":0,"23":0,"24":0,"25":0,"26":0,"27":0,"28":0,"29":0,"30":0,"31":0,"32":0,"33":0,"34":0,"35":0},"filename":{"0":"09_01_softmax_loss.py","1":"12_2_hello_rnn.py","2":"13_1_rnn_classification_basics.py","3":"13_2_rnn_classification.py","4":"13_2_rnn_classification.py","5":"13_2_rnn_classification.py","6":"13_3_char_rnn.py","7":"13_3_char_rnn.py","8":"13_3_char_rnn.py","9":"13_3_char_rnn.py","10":"14_1_seq2seq.py","11":"14_1_seq2seq.py","12":"14_1_seq2seq.py","13":"14_2_seq2seq_att.py","14":"14_2_seq2seq_att.py","15":"14_2_seq2seq_att.py","16":"name_dataset.py","17":"seq2seq_models.py","18":"text_loader.py","19":"text_loader.py","20":"text_loader.py","21":"text_loader.py","22":"text_loader.py","23":"text_loader.py","24":"text_loader.py","25":"text_loader.py","26":"text_loader.py","27":"text_loader.py","28":"text_loader.py","29":"text_loader.py","30":"text_loader.py","31":"text_loader.py","32":"text_loader.py","33":"text_loader.py","34":"text_loader.py","35":"text_loader.py"},"symbol":{"0":"redefined-builtin","1":"redefined-builtin","2":"redefined-builtin","3":"redefined-builtin","4":"redefined-builtin","5":"redefined-builtin","6":"redefined-builtin","7":"redefined-builtin","8":"consider-using-enumerate","9":"redefined-builtin","10":"consider-using-enumerate","11":"consider-using-enumerate","12":"literal-comparison","13":"consider-using-enumerate","14":"consider-using-enumerate","15":"literal-comparison","16":"redefined-builtin","17":"redefined-builtin","18":"duplicate-code","19":"duplicate-code","20":"duplicate-code","21":"duplicate-code","22":"duplicate-code","23":"duplicate-code","24":"duplicate-code","25":"duplicate-code","26":"duplicate-code","27":"duplicate-code","28":"duplicate-code","29":"duplicate-code","30":"duplicate-code","31":"duplicate-code","32":"duplicate-code","33":"duplicate-code","34":"duplicate-code","35":"duplicate-code"},"text":{"0":"Redefining built-in 'max'","1":"Redefining built-in 'input'","2":"Redefining built-in 'input'","3":"Redefining built-in 'input'","4":"Redefining built-in 'input'","5":"Redefining built-in 'input'","6":"Redefining built-in 'input'","7":"Redefining built-in 'input'","8":"Consider using enumerate instead of iterating with range and len","9":"Redefining built-in 'input'","10":"Consider using enumerate instead of iterating with range and len","11":"Consider using enumerate instead of iterating with range and len","12":"Comparison to literal","13":"Consider using enumerate instead of iterating with range and len","14":"Consider using enumerate instead of iterating with range and len","15":"Comparison to literal","16":"Redefining built-in 'id'","17":"Redefining built-in 'input'","18":"Similar lines in 2 files\n==10_1_cnn_mnist:1\n==11_1_toy_inception_mnist:1\nfrom __future__ import print_function\nimport argparse\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.autograd import Variable\n\n# Training settings\nbatch_size = 64\n\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='.\/data\/',\n                               train=True,\n                               transform=transforms.ToTensor(),\n                               download=True)\n\ntest_dataset = datasets.MNIST(root='.\/data\/',\n                              train=False,\n                              transform=transforms.ToTensor())\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                           batch_size=batch_size,\n                                           shuffle=True)\n\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n","19":"Similar lines in 2 files\n==07_diabets_logistic:10\n==08_2_dataset_loade_logistic:34\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate two nn.Linear module\n        \"\"\"\n        super(Model, self).__init__()\n        self.l1 = nn.Linear(8, 6)\n        self.l2 = nn.Linear(6, 4)\n        self.l3 = nn.Linear(4, 1)\n\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        \"\"\"\n        In the forward function we accept a Variable of input data and we must return\n        a Variable of output data. We can use Modules defined in the constructor as\n        well as arbitrary operators on Variables.\n        \"\"\"\n        out1 = self.sigmoid(self.l1(x))\n        out2 = self.sigmoid(self.l2(out1))\n        y_pred = self.sigmoid(self.l3(out2))\n        return y_pred\n\n\n# our model\nmodel = Model()\n\n# Construct our loss function and an Optimizer. The call to model.parameters()\n# in the SGD constructor will contain the learnable parameters of the two\n# nn.Linear modules which are members of the model.","20":"Similar lines in 2 files\n==10_1_cnn_mnist:46\n==11_1_toy_inception_mnist:84\n        x = x.view(in_size, -1)  # flatten the tensor\n        x = self.fc(x)\n        return F.log_softmax(x)\n\n\nmodel = Net()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n\n\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = Variable(data), Variable(target)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 10 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),","21":"Similar lines in 2 files\n==14_1_seq2seq:52\n==14_2_seq2seq_att:68\n        loss += criterion(output, target_var[c])\n\n    encoder.zero_grad()\n    decoder.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    return loss.data[0] \/ len(target_var)\n\n\n# Translate the given input\ndef translate(enc_input='thisissungkim.iloveyou.', predict_len=100, temperature=0.9):\n    input_var = str2tensor(enc_input)\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(input_var, encoder_hidden)\n\n    hidden = encoder_hidden\n\n    predicted = ''\n    dec_input = str2tensor(SOS_token)","22":"Similar lines in 2 files\n==08_1_dataset_loader:11\n==08_2_dataset_loade_logistic:11\n    def __init__(self):\n        xy = np.loadtxt('.\/data\/diabetes.csv.gz',\n                        delimiter=',', dtype=np.float32)\n        self.len = xy.shape[0]\n        self.x_data = from_numpy(xy[:, 0:-1])\n        self.y_data = from_numpy(xy[:, [-1]])\n\n    def __getitem__(self, index):\n        return self.x_data[index], self.y_data[index]\n\n    def __len__(self):\n        return self.len\n\n\ndataset = DiabetesDataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=32,\n                          shuffle=True,\n                          num_workers=2)\n","23":"Similar lines in 2 files\n==13_1_rnn_classification_basics:1\n==13_2_rnn_classification:1\nimport time\nimport math\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\n\nfrom name_dataset import NameDataset\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n\n# Parameters and DataLoaders\nHIDDEN_SIZE = 100","24":"Similar lines in 2 files\n==10_1_cnn_mnist:81\n==11_1_toy_inception_mnist:119\n        pred = output.data.max(1, keepdim=True)[1]\n        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n\n    test_loss \/= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct \/ len(test_loader.dataset)))\n\n\nfor epoch in range(1, 10):\n    train(epoch)\n    test()","25":"Similar lines in 2 files\n==14_1_seq2seq:105\n==14_2_seq2seq_att:122\ntrain_loader = DataLoader(dataset=TextDataset(),\n                          batch_size=BATCH_SIZE,\n                          shuffle=True,\n                          num_workers=2)\n\nprint(\"Training for %d epochs...\" % N_EPOCH)\nfor epoch in range(1, N_EPOCH + 1):\n    # Get srcs and targets from data loader\n    for i, (srcs, targets) in enumerate(train_loader):\n        train_loss = train(srcs[0], targets[0])  # Batch is 1\n","26":"Similar lines in 2 files\n==12_3_hello_rnn_seq:1\n==12_4_hello_rnn_emb:1\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\ntorch.manual_seed(777)  # reproducibility\n\n\nidx2char = ['h', 'i', 'e', 'l', 'o']\n\n# Teach hihell -> ihello\nx_data = [[0, 1, 0, 2, 3, 3]]   # hihell","27":"Similar lines in 2 files\n==01_basics:8\n==02_manual_gradient:8\ndef forward(x):\n    return x * w\n\n\n# Loss function\ndef loss(x, y):\n    y_pred = forward(x)\n    return (y_pred - y) * (y_pred - y)\n\n","28":"Similar lines in 2 files\n==09_2_softmax_mnist:69\n==10_1_cnn_mnist:67\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.item()))\n\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:","29":"Similar lines in 2 files\n==14_1_seq2seq:80\n==14_2_seq2seq_att:97\n        if top_i is EOS_token:\n            break\n\n        predicted_char = chr(top_i)\n        predicted += predicted_char\n\n        dec_input = str2tensor(predicted_char)\n","30":"Similar lines in 2 files\n==10_1_cnn_mnist:71\n==11_1_toy_inception_mnist:109\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:\n        data, target = Variable(data, volatile=True), Variable(target)\n        output = model(data)\n        # sum up batch loss","31":"Similar lines in 2 files\n==09_2_softmax_mnist:29\n==10_1_cnn_mnist:29\n                                          batch_size=batch_size,\n                                          shuffle=False)\n\n\nclass Net(nn.Module):\n\n    def __init__(self):\n        super(Net, self).__init__()","32":"Similar lines in 2 files\n==14_1_seq2seq:38\n==14_2_seq2seq_att:56\n    src_var = str2tensor(src)\n    target_var = str2tensor(target, eos=True)  # Add the EOS token\n\n    encoder_hidden = encoder.init_hidden()\n    encoder_outputs, encoder_hidden = encoder(src_var, encoder_hidden)\n\n    hidden = encoder_hidden","33":"Similar lines in 2 files\n==12_3_hello_rnn_seq:75\n==12_4_hello_rnn_emb:66\n    optimizer.zero_grad()\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    _, idx = outputs.max(1)\n    idx = idx.data.numpy()\n    result_str = [idx2char[c] for c in idx.squeeze()]","34":"Similar lines in 2 files\n==12_2_hello_rnn:22\n==12_3_hello_rnn_seq:22\ninputs = Variable(torch.Tensor(x_one_hot))\nlabels = Variable(torch.LongTensor(y_data))\n\nnum_classes = 5\ninput_size = 5  # one-hot size\nhidden_size = 5  # output from the RNN. 5 to directly predict one-hot\nbatch_size = 1   # one sentence","35":"Similar lines in 2 files\n==09_2_softmax_mnist:73\n==11_1_toy_inception_mnist:109\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    for data, target in test_loader:"},"number":{"0":"W0622","1":"W0622","2":"W0622","3":"W0622","4":"W0622","5":"W0622","6":"W0622","7":"W0622","8":"C0200","9":"W0622","10":"C0200","11":"C0200","12":"R0123","13":"C0200","14":"C0200","15":"R0123","16":"W0622","17":"W0622","18":"R0801","19":"R0801","20":"R0801","21":"R0801","22":"R0801","23":"R0801","24":"R0801","25":"R0801","26":"R0801","27":"R0801","28":"R0801","29":"R0801","30":"R0801","31":"R0801","32":"R0801","33":"R0801","34":"R0801","35":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint","17":"pylint","18":"pylint","19":"pylint","20":"pylint","21":"pylint","22":"pylint","23":"pylint","24":"pylint","25":"pylint","26":"pylint","27":"pylint","28":"pylint","29":"pylint","30":"pylint","31":"pylint","32":"pylint","33":"pylint","34":"pylint","35":"pylint"},"lines_amount":{"0":53,"1":86,"2":96,"3":213,"4":213,"5":213,"6":148,"7":148,"8":148,"9":148,"10":122,"11":122,"12":122,"13":146,"14":146,"15":146,"16":59,"17":144,"18":36,"19":36,"20":36,"21":36,"22":36,"23":36,"24":36,"25":36,"26":36,"27":36,"28":36,"29":36,"30":36,"31":36,"32":36,"33":36,"34":36,"35":36},"commit":{"0":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","1":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","2":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","3":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","4":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","5":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","6":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","7":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","8":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","9":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","10":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","11":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","12":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","13":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","14":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","15":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","16":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","17":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","18":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","19":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","20":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","21":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","22":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","23":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","24":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","25":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","26":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","27":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","28":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","29":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","30":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","31":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","32":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","33":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","34":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289","35":"e5a260fac0b4ff1e2ec434fc6f0d962ac97fc289"},"repo":{"0":"hunkim\/PyTorchZeroToAll","1":"hunkim\/PyTorchZeroToAll","2":"hunkim\/PyTorchZeroToAll","3":"hunkim\/PyTorchZeroToAll","4":"hunkim\/PyTorchZeroToAll","5":"hunkim\/PyTorchZeroToAll","6":"hunkim\/PyTorchZeroToAll","7":"hunkim\/PyTorchZeroToAll","8":"hunkim\/PyTorchZeroToAll","9":"hunkim\/PyTorchZeroToAll","10":"hunkim\/PyTorchZeroToAll","11":"hunkim\/PyTorchZeroToAll","12":"hunkim\/PyTorchZeroToAll","13":"hunkim\/PyTorchZeroToAll","14":"hunkim\/PyTorchZeroToAll","15":"hunkim\/PyTorchZeroToAll","16":"hunkim\/PyTorchZeroToAll","17":"hunkim\/PyTorchZeroToAll","18":"hunkim\/PyTorchZeroToAll","19":"hunkim\/PyTorchZeroToAll","20":"hunkim\/PyTorchZeroToAll","21":"hunkim\/PyTorchZeroToAll","22":"hunkim\/PyTorchZeroToAll","23":"hunkim\/PyTorchZeroToAll","24":"hunkim\/PyTorchZeroToAll","25":"hunkim\/PyTorchZeroToAll","26":"hunkim\/PyTorchZeroToAll","27":"hunkim\/PyTorchZeroToAll","28":"hunkim\/PyTorchZeroToAll","29":"hunkim\/PyTorchZeroToAll","30":"hunkim\/PyTorchZeroToAll","31":"hunkim\/PyTorchZeroToAll","32":"hunkim\/PyTorchZeroToAll","33":"hunkim\/PyTorchZeroToAll","34":"hunkim\/PyTorchZeroToAll","35":"hunkim\/PyTorchZeroToAll"},"stargazers":{"0":3564,"1":3564,"2":3564,"3":3564,"4":3564,"5":3564,"6":3564,"7":3564,"8":3564,"9":3564,"10":3564,"11":3564,"12":3564,"13":3564,"14":3564,"15":3564,"16":3564,"17":3564,"18":3564,"19":3564,"20":3564,"21":3564,"22":3564,"23":3564,"24":3564,"25":3564,"26":3564,"27":3564,"28":3564,"29":3564,"30":3564,"31":3564,"32":3564,"33":3564,"34":3564,"35":3564}}