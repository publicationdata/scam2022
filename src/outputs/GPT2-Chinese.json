{"type":{"0":"W","1":"W","2":"R","3":"W","4":"R","5":"W","6":"W","7":"R","8":"R","9":"R","10":"R","11":"R","12":"R","13":"R","14":"R","15":"R","16":"R","17":"R","18":"R","19":"R","20":"R","21":"R","22":"R","23":"R","24":"R","25":"R","26":"R","27":"R","28":"R","29":"R","30":"R","31":"R","32":"R"},"module":{"0":"eval","1":"generate","2":"generate","3":"generate_texts","4":"generate_texts","5":"train","6":"train_single","7":"train_single","8":"train_single","9":"train_single","10":"train_single","11":"train_single","12":"train_single","13":"train_single","14":"train_single","15":"train_single","16":"train_single","17":"train_single","18":"train_single","19":"train_single","20":"train_single","21":"train_single","22":"train_single","23":"train_single","24":"train_single","25":"train_single","26":"train_single","27":"train_single","28":"train_single","29":"train_single","30":"train_single","31":"train_single","32":"train_single"},"obj":{"0":"build_files","1":"sample_sequence","2":"main","3":"sample_sequence","4":"main","5":"build_files","6":"build_files","7":"","8":"","9":"","10":"","11":"","12":"","13":"","14":"","15":"","16":"","17":"","18":"","19":"","20":"","21":"","22":"","23":"","24":"","25":"","26":"","27":"","28":"","29":"","30":"","31":"","32":""},"lnum":{"0":34,"1":82,"2":176,"3":84,"4":174,"5":36,"6":30,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1},"col":{"0":16,"1":16,"2":23,"3":16,"4":23,"5":16,"6":16,"7":0,"8":0,"9":0,"10":0,"11":0,"12":0,"13":0,"14":0,"15":0,"16":0,"17":0,"18":0,"19":0,"20":0,"21":0,"22":0,"23":0,"24":0,"25":0,"26":0,"27":0,"28":0,"29":0,"30":0,"31":0,"32":0},"filename":{"0":"eval.py","1":"generate.py","2":"generate.py","3":"generate_texts.py","4":"generate_texts.py","5":"train.py","6":"train_single.py","7":"train_single.py","8":"train_single.py","9":"train_single.py","10":"train_single.py","11":"train_single.py","12":"train_single.py","13":"train_single.py","14":"train_single.py","15":"train_single.py","16":"train_single.py","17":"train_single.py","18":"train_single.py","19":"train_single.py","20":"train_single.py","21":"train_single.py","22":"train_single.py","23":"train_single.py","24":"train_single.py","25":"train_single.py","26":"train_single.py","27":"train_single.py","28":"train_single.py","29":"train_single.py","30":"train_single.py","31":"train_single.py","32":"train_single.py"},"symbol":{"0":"redefined-builtin","1":"redefined-builtin","2":"consider-using-with","3":"redefined-builtin","4":"consider-using-in","5":"redefined-builtin","6":"redefined-builtin","7":"duplicate-code","8":"duplicate-code","9":"duplicate-code","10":"duplicate-code","11":"duplicate-code","12":"duplicate-code","13":"duplicate-code","14":"duplicate-code","15":"duplicate-code","16":"duplicate-code","17":"duplicate-code","18":"duplicate-code","19":"duplicate-code","20":"duplicate-code","21":"duplicate-code","22":"duplicate-code","23":"duplicate-code","24":"duplicate-code","25":"duplicate-code","26":"duplicate-code","27":"duplicate-code","28":"duplicate-code","29":"duplicate-code","30":"duplicate-code","31":"duplicate-code","32":"duplicate-code"},"text":{"0":"Redefining built-in 'id'","1":"Redefining built-in 'id'","2":"Consider using 'with' for resource-allocating operations","3":"Redefining built-in 'id'","4":"Consider merging these comparisons with \"in\" to \"item in ('[CLS]', '[SEP]')\"","5":"Redefining built-in 'id'","6":"Redefining built-in 'id'","7":"Similar lines in 2 files\n==generate:8\n==generate_texts:10\ndef is_word(word):\n    for item in list(word):\n        if item not in 'qwertyuiopasdfghjklzxcvbnm':\n            return False\n    return True\n\n\ndef _is_chinese_char(char):\n    \"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\n    # This defines a \"chinese character\" as anything in the CJK Unicode block:\n    #   https:\/\/en.wikipedia.org\/wiki\/CJK_Unified_Ideographs_(Unicode_block)\n    #\n    # Note that the CJK Unicode block is NOT all Japanese and Korean characters,\n    # despite its name. The modern Korean Hangul alphabet is a different block,\n    # as is Japanese Hiragana and Katakana. Those alphabets are used to write\n    # space-separated words, so they are not treated specially and handled\n    # like the all of the other languages.\n    cp = ord(char)\n    if ((cp >= 0x4E00 and cp <= 0x9FFF) or  #\n            (cp >= 0x3400 and cp <= 0x4DBF) or  #\n            (cp >= 0x20000 and cp <= 0x2A6DF) or  #\n            (cp >= 0x2A700 and cp <= 0x2B73F) or  #\n            (cp >= 0x2B740 and cp <= 0x2B81F) or  #\n            (cp >= 0x2B820 and cp <= 0x2CEAF) or\n            (cp >= 0xF900 and cp <= 0xFAFF) or  #\n            (cp >= 0x2F800 and cp <= 0x2FA1F)):  #\n        return True\n\n    return False\n\n\ndef top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n    \"\"\" Filter a distribution of logits using top-k and\/or nucleus (top-p) filtering\n        Args:\n            logits: logits distribution shape (vocabulary size)\n            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n                Nucleus filtering is described in Holtzman et al. (http:\/\/arxiv.org\/abs\/1904.09751)\n        From: https:\/\/gist.github.com\/thomwolf\/1a5a29f6962089e871b94cbd09daf317\n    \"\"\"\n    assert logits.dim() == 1  # batch size 1 for now - could be updated for more but the code would be less clear\n    top_k = min(top_k, logits.size(-1))  # Safety check\n    if top_k > 0:\n        # Remove all tokens with a probability less than the last token of the top-k\n        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n        logits[indices_to_remove] = filter_value\n\n    if top_p > 0.0:\n        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n\n        # Remove tokens with cumulative probability above the threshold\n        sorted_indices_to_remove = cumulative_probs > top_p\n        # Shift the indices to the right to keep also the first token above the threshold\n        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n        sorted_indices_to_remove[..., 0] = 0\n\n        indices_to_remove = sorted_indices[sorted_indices_to_remove]\n        logits[indices_to_remove] = filter_value\n    return logits\n\n\ndef sample_sequence(model, context, length, n_ctx, tokenizer, temperature=1.0, top_k=30, top_p=0.0, repitition_penalty=1.0,\n                    device='cpu'):\n    context = torch.tensor(context, dtype=torch.long, device=device)\n    context = context.unsqueeze(0)\n    generated = context\n    with torch.no_grad():\n        for _ in trange(length):\n            inputs = {'input_ids': generated[0][-(n_ctx - 1):].unsqueeze(0)}\n            outputs = model(\n                **inputs)  # Note: we could also use 'past' with GPT-2\/Transfo-XL\/XLNet (cached hidden-states)\n            next_token_logits = outputs[0][0, -1, :]\n            for id in set(generated):\n                next_token_logits[id] \/= repitition_penalty\n            next_token_logits = next_token_logits \/ temperature\n            next_token_logits[tokenizer.convert_tokens_to_ids('[UNK]')] = -float('Inf')\n            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n            next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n            generated = torch.cat((generated, next_token.unsqueeze(0)), dim=1)","8":"Similar lines in 2 files\n==train:225\n==train_single:201\n            piece_num += 1\n\n        print('saving model for epoch {}'.format(epoch + 1))\n        if not os.path.exists(output_dir + 'model_epoch{}'.format(epoch + 1)):\n            os.mkdir(output_dir + 'model_epoch{}'.format(epoch + 1))\n        model_to_save = model.module if hasattr(model, 'module') else model\n        model_to_save.save_pretrained(output_dir + 'model_epoch{}'.format(epoch + 1))\n        # torch.save(scheduler.state_dict(), output_dir + 'model_epoch{}\/scheduler.pt'.format(epoch + 1))\n        # torch.save(optimizer.state_dict(), output_dir + 'model_epoch{}\/optimizer.pt'.format(epoch + 1))\n        print('epoch {} finished'.format(epoch + 1))\n\n        then = datetime.now()\n        print('time: {}'.format(then))\n        print('time for one epoch: {}'.format(then - now))\n\n    print('training finished')\n    if not os.path.exists(output_dir + 'final_model'):\n        os.mkdir(output_dir + 'final_model')\n    model_to_save = model.module if hasattr(model, 'module') else model\n    model_to_save.save_pretrained(output_dir + 'final_model')\n    # torch.save(scheduler.state_dict(), output_dir + 'final_model\/scheduler.pt')\n    # torch.save(optimizer.state_dict(), output_dir + 'final_model\/optimizer.pt')\n\n\nif __name__ == '__main__':\n    main()","9":"Similar lines in 2 files\n==train:158\n==train_single:132\n    running_loss = 0\n    for epoch in range(epochs):\n        print('epoch {}'.format(epoch + 1))\n        now = datetime.now()\n        print('time: {}'.format(now))\n        x = np.linspace(0, num_pieces - 1, num_pieces, dtype=np.int32)\n        random.shuffle(x)\n        piece_num = 0\n        for i in x:\n            with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'r') as f:\n                line = f.read().strip()\n            tokens = line.split()\n            tokens = [int(token) for token in tokens]\n            start_point = 0\n            samples = []\n            while start_point < len(tokens) - n_ctx:\n                samples.append(tokens[start_point: start_point + n_ctx])\n                start_point += stride\n            if start_point < len(tokens):\n                samples.append(tokens[len(tokens)-n_ctx:])\n            random.shuffle(samples)\n            for step in range(len(samples) \/\/ batch_size):\n\n                #  prepare data\n                batch = samples[step * batch_size: (step + 1) * batch_size]","10":"Similar lines in 2 files\n==eval:20\n==train:22\n    for i in tqdm(range(num_pieces)):\n        sublines = lines[all_len \/\/ num_pieces * i: all_len \/\/ num_pieces * (i + 1)]\n        if i == num_pieces - 1:\n            sublines.extend(lines[all_len \/\/ num_pieces * (i + 1):])  # \u628a\u5c3e\u90e8\u4f8b\u5b50\u6dfb\u52a0\u5230\u6700\u540e\u4e00\u4e2apiece\n        sublines = [full_tokenizer.tokenize(line) for line in sublines if\n                    len(line) > min_length]  # \u53ea\u8003\u8651\u957f\u5ea6\u8d85\u8fc7min_length\u7684\u53e5\u5b50\n        sublines = [full_tokenizer.convert_tokens_to_ids(line) for line in sublines]\n        full_line = []\n        for subline in sublines:\n            full_line.append(full_tokenizer.convert_tokens_to_ids('[MASK]'))  # \u6587\u7ae0\u5f00\u5934\u6dfb\u52a0MASK\u8868\u793a\u6587\u7ae0\u5f00\u59cb\n            full_line.extend(subline)\n            full_line.append(full_tokenizer.convert_tokens_to_ids('[CLS]'))  # \u6587\u7ae0\u4e4b\u95f4\u6dfb\u52a0CLS\u8868\u793a\u6587\u7ae0\u7ed3\u675f\n        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'w') as f:\n            for id in full_line:\n                f.write(str(id) + ' ')\n    print('finish')\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--device', default='0,1,2,3', type=str, required=False, help='\u8bbe\u7f6e\u4f7f\u7528\u54ea\u4e9b\u663e\u5361')\n    parser.add_argument('--model_config', default='config\/model_config_small.json', type=str, required=False,\n                        help='\u9009\u62e9\u6a21\u578b\u53c2\u6570')\n    parser.add_argument('--tokenizer_path', default='cache\/vocab_small.txt', type=str, required=False, help='\u9009\u62e9\u8bcd\u5e93')","11":"Similar lines in 2 files\n==eval:141\n==train_single:152\n            random.shuffle(samples)\n            for step in range(len(samples) \/\/ batch_size):\n\n                #  prepare data\n                batch = samples[step * batch_size: (step + 1) * batch_size]\n                batch_labels = []\n                batch_inputs = []\n                for ids in batch:\n                    int_ids_for_labels = [int(x) for x in ids]\n                    int_ids_for_inputs = [int(x) for x in ids]\n                    batch_labels.append(int_ids_for_labels)\n                    batch_inputs.append(int_ids_for_inputs)\n                batch_labels = torch.tensor(batch_labels).long().to(device)\n                batch_inputs = torch.tensor(batch_inputs).long().to(device)\n\n                #  forward pass\n                outputs = model.forward(input_ids=batch_inputs, labels=batch_labels)\n                loss, logits = outputs[:2]\n\n                #  get loss\n                if multi_gpu:\n                    loss = loss.mean()","12":"Similar lines in 2 files\n==train:133\n==train_single:108\n    multi_gpu = False\n    full_len = 0\n    print('calculating total steps')\n    for i in tqdm(range(num_pieces)):\n        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'r') as f:\n            full_len += len([int(item) for item in f.read().strip().split()])\n    total_steps = int(full_len \/ stride * epochs \/ batch_size \/ gradient_accumulation)\n    print('total steps = {}'.format(total_steps))\n\n    optimizer = transformers.AdamW(model.parameters(), lr=lr, correct_bias=True)\n    scheduler = transformers.WarmupLinearSchedule(optimizer, warmup_steps=warmup_steps,\n                                                          t_total=total_steps)\n    if fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https:\/\/www.github.com\/nvidia\/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=fp16_opt_level)\n\n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")","13":"Similar lines in 2 files\n==train:87\n==train_single:75\n        full_tokenizer = tokenization_bert.BertTokenizer(vocab_file=args.tokenizer_path)\n    full_tokenizer.max_len = 999999\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print('using device:', device)\n\n    raw_data_path = args.raw_data_path\n    tokenized_data_path = args.tokenized_data_path\n    raw = args.raw  # \u9009\u62e9\u662f\u5426\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u6570\u636e\u96c6\n    epochs = args.epochs\n    batch_size = args.batch_size\n    lr = args.lr\n    warmup_steps = args.warmup_steps\n    log_step = args.log_step\n    stride = args.stride\n    gradient_accumulation = args.gradient_accumulation\n    fp16 = args.fp16  # \u4e0d\u652f\u6301\u534a\u7cbe\u5ea6\u7684\u663e\u5361\u8bf7\u52ff\u6253\u5f00\n    fp16_opt_level = args.fp16_opt_level\n    max_grad_norm = args.max_grad_norm\n    num_pieces = args.num_pieces","14":"Similar lines in 2 files\n==train:191\n==train_single:169\n                loss, logits = outputs[:2]\n\n                #  get loss\n                if multi_gpu:\n                    loss = loss.mean()\n                if gradient_accumulation > 1:\n                    loss = loss \/ gradient_accumulation\n\n                #  loss backward\n                if fp16:\n                    with amp.scale_loss(loss, optimizer) as scaled_loss:\n                        scaled_loss.backward()\n                        torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), max_grad_norm)\n                else:\n                    loss.backward()\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n\n                #  optimizer step","15":"Similar lines in 2 files\n==train:37\n==train_single:34\n    print('finish')\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--device', default='0,1,2,3', type=str, required=False, help='\u8bbe\u7f6e\u4f7f\u7528\u54ea\u4e9b\u663e\u5361')\n    parser.add_argument('--model_config', default='config\/model_config_small.json', type=str, required=False,\n                        help='\u9009\u62e9\u6a21\u578b\u53c2\u6570')\n    parser.add_argument('--tokenizer_path', default='cache\/vocab_small.txt', type=str, required=False, help='\u9009\u62e9\u8bcd\u5e93')\n    parser.add_argument('--raw_data_path', default='data\/train.json', type=str, required=False, help='\u539f\u59cb\u8bad\u7ec3\u8bed\u6599')\n    parser.add_argument('--tokenized_data_path', default='data\/tokenized\/', type=str, required=False,\n                        help='tokenized\u8bed\u6599\u5b58\u653e\u4f4d\u7f6e')\n    parser.add_argument('--raw', action='store_true', help='\u662f\u5426\u5148\u505atokenize')\n    parser.add_argument('--epochs', default=5, type=int, required=False, help='\u8bad\u7ec3\u5faa\u73af')\n    parser.add_argument('--batch_size', default=8, type=int, required=False, help='\u8bad\u7ec3batch size')\n    parser.add_argument('--lr', default=1.5e-4, type=float, required=False, help='\u5b66\u4e60\u7387')\n    parser.add_argument('--warmup_steps', default=2000, type=int, required=False, help='warm up\u6b65\u6570')","16":"Similar lines in 2 files\n==eval:100\n==train:125\n    model.to(device)\n\n    num_parameters = 0\n    parameters = model.parameters()\n    for parameter in parameters:\n        num_parameters += parameter.numel()\n    print('number of parameters: {}'.format(num_parameters))\n\n    multi_gpu = False\n    full_len = 0\n    print('calculating total steps')\n    for i in tqdm(range(num_pieces)):\n        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'r') as f:\n            full_len += len([int(item) for item in f.read().strip().split()])","17":"Similar lines in 2 files\n==generate:142\n==generate_texts:111\n    parser.add_argument('--repetition_penalty', default=1.0, type=float, required=False)\n\n    args = parser.parse_args()\n    print('args:\\n' + args.__repr__())\n\n    if args.segment:\n        from tokenizations import tokenization_bert_word_level as tokenization_bert\n    else:\n        from tokenizations import tokenization_bert\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device  # \u6b64\u5904\u8bbe\u7f6e\u7a0b\u5e8f\u4f7f\u7528\u54ea\u4e9b\u663e\u5361\n    length = args.length","18":"Similar lines in 2 files\n==eval:85\n==train:111\n    if not os.path.exists(output_dir):\n        os.mkdir(output_dir)\n\n    if raw:\n        print('building files')\n        build_files(data_path=raw_data_path, tokenized_data_path=tokenized_data_path, num_pieces=num_pieces,\n                    full_tokenizer=full_tokenizer, min_length=min_length)\n        print('files built')\n\n    if not args.pretrained_model:","19":"Similar lines in 4 files\n==generate:144\n==generate_texts:113\n==train:70\n==train_single:62\n    args = parser.parse_args()\n    print('args:\\n' + args.__repr__())\n\n    if args.segment:\n        from tokenizations import tokenization_bert_word_level as tokenization_bert\n    else:\n        from tokenizations import tokenization_bert\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device  # \u6b64\u5904\u8bbe\u7f6e\u7a0b\u5e8f\u4f7f\u7528\u54ea\u4e9b\u663e\u5361","20":"Similar lines in 2 files\n==generate:161\n==generate_texts:135\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    tokenizer = tokenization_bert.BertTokenizer(vocab_file=args.tokenizer_path)\n    model = GPT2LMHeadModel.from_pretrained(args.model_path)\n    model.to(device)\n    model.eval()\n\n    n_ctx = model.config.n_ctx\n","21":"Similar lines in 2 files\n==eval:35\n==train_single:34\n    print('finish')\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--device', default='0,1,2,3', type=str, required=False, help='\u8bbe\u7f6e\u4f7f\u7528\u54ea\u4e9b\u663e\u5361')\n    parser.add_argument('--model_config', default='config\/model_config_small.json', type=str, required=False,\n                        help='\u9009\u62e9\u6a21\u578b\u53c2\u6570')\n    parser.add_argument('--tokenizer_path', default='cache\/vocab_small.txt', type=str, required=False, help='\u9009\u62e9\u8bcd\u5e93')","22":"Similar lines in 3 files\n==eval:129\n==train:167\n==train_single:141\n            with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'r') as f:\n                line = f.read().strip()\n            tokens = line.split()\n            tokens = [int(token) for token in tokens]\n            start_point = 0\n            samples = []\n            while start_point < len(tokens) - n_ctx:\n                samples.append(tokens[start_point: start_point + n_ctx])\n                start_point += stride","23":"Similar lines in 2 files\n==train:118\n==train_single:100\n        print('files built')\n\n    if not args.pretrained_model:\n        model = transformers.modeling_gpt2.GPT2LMHeadModel(config=model_config)\n    else:\n        model = transformers.modeling_gpt2.GPT2LMHeadModel.from_pretrained(args.pretrained_model)\n    model.train()\n    model.to(device)","24":"Similar lines in 2 files\n==eval:62\n==train:76\n        from tokenizations import tokenization_bert\n\n    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.device  # \u6b64\u5904\u8bbe\u7f6e\u7a0b\u5e8f\u4f7f\u7528\u54ea\u4e9b\u663e\u5361\n\n    model_config = transformers.modeling_gpt2.GPT2Config.from_json_file(args.model_config)\n    print('config:\\n' + model_config.to_json_string())\n\n    n_ctx = model_config.n_ctx","25":"Similar lines in 2 files\n==generate:0\n==generate_texts:0\nimport torch\nimport torch.nn.functional as F\nimport os\nimport argparse\nfrom tqdm import trange\nfrom transformers import GPT2LMHeadModel\n","26":"Similar lines in 2 files\n==eval:0\n==train:0\nimport transformers\nimport torch\nimport os\nimport json\nimport random\nimport numpy as np\nimport argparse","27":"Similar lines in 2 files\n==train:55\n==train_single:52\n    parser.add_argument('--stride', default=768, type=int, required=False, help='\u8bad\u7ec3\u65f6\u53d6\u8bad\u7ec3\u6570\u636e\u7684\u7a97\u53e3\u6b65\u957f')\n    parser.add_argument('--gradient_accumulation', default=1, type=int, required=False, help='\u68af\u5ea6\u79ef\u7d2f')\n    parser.add_argument('--fp16', action='store_true', help='\u6df7\u5408\u7cbe\u5ea6')\n    parser.add_argument('--fp16_opt_level', default='O1', type=str, required=False)\n    parser.add_argument('--max_grad_norm', default=1.0, type=float, required=False)\n    parser.add_argument('--num_pieces', default=100, type=int, required=False, help='\u5c06\u8bad\u7ec3\u8bed\u6599\u5206\u6210\u591a\u5c11\u4efd')","28":"Similar lines in 2 files\n==eval:108\n==train_single:108\n    multi_gpu = False\n    full_len = 0\n    print('calculating total steps')\n    for i in tqdm(range(num_pieces)):\n        with open(tokenized_data_path + 'tokenized_train_{}.txt'.format(i), 'r') as f:\n            full_len += len([int(item) for item in f.read().strip().split()])","29":"Similar lines in 3 files\n==eval:72\n==train:89\n==train_single:77\n    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n    print('using device:', device)\n\n    raw_data_path = args.raw_data_path\n    tokenized_data_path = args.tokenized_data_path\n    raw = args.raw  # \u9009\u62e9\u662f\u5426\u4ece\u96f6\u5f00\u59cb\u6784\u5efa\u6570\u636e\u96c6","30":"Similar lines in 2 files\n==eval:115\n==train_single:127\n    if torch.cuda.device_count() > 1:\n        print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n        model = DataParallel(model)\n        multi_gpu = True\n    print('starting training')","31":"Similar lines in 3 files\n==eval:0\n==train:0\n==train_single:0\nimport transformers\nimport torch\nimport os\nimport json\nimport random","32":"Similar lines in 2 files\n==eval:15\n==train:15\n    with open(data_path, 'r', encoding='utf8') as f:\n        print('reading lines')\n        lines = json.load(f)\n        lines = [line.replace('\\n', ' [SEP] ') for line in lines]  # \u7528[SEP]\u8868\u793a\u6362\u884c, \u6bb5\u843d\u4e4b\u95f4\u4f7f\u7528SEP\u8868\u793a\u6bb5\u843d\u7ed3\u675f\n        all_len = len(lines)"},"number":{"0":"W0622","1":"W0622","2":"R1732","3":"W0622","4":"R1714","5":"W0622","6":"W0622","7":"R0801","8":"R0801","9":"R0801","10":"R0801","11":"R0801","12":"R0801","13":"R0801","14":"R0801","15":"R0801","16":"R0801","17":"R0801","18":"R0801","19":"R0801","20":"R0801","21":"R0801","22":"R0801","23":"R0801","24":"R0801","25":"R0801","26":"R0801","27":"R0801","28":"R0801","29":"R0801","30":"R0801","31":"R0801","32":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint","17":"pylint","18":"pylint","19":"pylint","20":"pylint","21":"pylint","22":"pylint","23":"pylint","24":"pylint","25":"pylint","26":"pylint","27":"pylint","28":"pylint","29":"pylint","30":"pylint","31":"pylint","32":"pylint"},"lines_amount":{"0":185,"1":223,"2":223,"3":187,"4":187,"5":252,"6":228,"7":228,"8":228,"9":228,"10":228,"11":228,"12":228,"13":228,"14":228,"15":228,"16":228,"17":228,"18":228,"19":228,"20":228,"21":228,"22":228,"23":228,"24":228,"25":228,"26":228,"27":228,"28":228,"29":228,"30":228,"31":228,"32":228},"commit":{"0":"bbb44651be8361faef35d2a857451d231b5ebe14","1":"bbb44651be8361faef35d2a857451d231b5ebe14","2":"bbb44651be8361faef35d2a857451d231b5ebe14","3":"bbb44651be8361faef35d2a857451d231b5ebe14","4":"bbb44651be8361faef35d2a857451d231b5ebe14","5":"bbb44651be8361faef35d2a857451d231b5ebe14","6":"bbb44651be8361faef35d2a857451d231b5ebe14","7":"bbb44651be8361faef35d2a857451d231b5ebe14","8":"bbb44651be8361faef35d2a857451d231b5ebe14","9":"bbb44651be8361faef35d2a857451d231b5ebe14","10":"bbb44651be8361faef35d2a857451d231b5ebe14","11":"bbb44651be8361faef35d2a857451d231b5ebe14","12":"bbb44651be8361faef35d2a857451d231b5ebe14","13":"bbb44651be8361faef35d2a857451d231b5ebe14","14":"bbb44651be8361faef35d2a857451d231b5ebe14","15":"bbb44651be8361faef35d2a857451d231b5ebe14","16":"bbb44651be8361faef35d2a857451d231b5ebe14","17":"bbb44651be8361faef35d2a857451d231b5ebe14","18":"bbb44651be8361faef35d2a857451d231b5ebe14","19":"bbb44651be8361faef35d2a857451d231b5ebe14","20":"bbb44651be8361faef35d2a857451d231b5ebe14","21":"bbb44651be8361faef35d2a857451d231b5ebe14","22":"bbb44651be8361faef35d2a857451d231b5ebe14","23":"bbb44651be8361faef35d2a857451d231b5ebe14","24":"bbb44651be8361faef35d2a857451d231b5ebe14","25":"bbb44651be8361faef35d2a857451d231b5ebe14","26":"bbb44651be8361faef35d2a857451d231b5ebe14","27":"bbb44651be8361faef35d2a857451d231b5ebe14","28":"bbb44651be8361faef35d2a857451d231b5ebe14","29":"bbb44651be8361faef35d2a857451d231b5ebe14","30":"bbb44651be8361faef35d2a857451d231b5ebe14","31":"bbb44651be8361faef35d2a857451d231b5ebe14","32":"bbb44651be8361faef35d2a857451d231b5ebe14"},"repo":{"0":"Morizeyao\/GPT2-Chinese","1":"Morizeyao\/GPT2-Chinese","2":"Morizeyao\/GPT2-Chinese","3":"Morizeyao\/GPT2-Chinese","4":"Morizeyao\/GPT2-Chinese","5":"Morizeyao\/GPT2-Chinese","6":"Morizeyao\/GPT2-Chinese","7":"Morizeyao\/GPT2-Chinese","8":"Morizeyao\/GPT2-Chinese","9":"Morizeyao\/GPT2-Chinese","10":"Morizeyao\/GPT2-Chinese","11":"Morizeyao\/GPT2-Chinese","12":"Morizeyao\/GPT2-Chinese","13":"Morizeyao\/GPT2-Chinese","14":"Morizeyao\/GPT2-Chinese","15":"Morizeyao\/GPT2-Chinese","16":"Morizeyao\/GPT2-Chinese","17":"Morizeyao\/GPT2-Chinese","18":"Morizeyao\/GPT2-Chinese","19":"Morizeyao\/GPT2-Chinese","20":"Morizeyao\/GPT2-Chinese","21":"Morizeyao\/GPT2-Chinese","22":"Morizeyao\/GPT2-Chinese","23":"Morizeyao\/GPT2-Chinese","24":"Morizeyao\/GPT2-Chinese","25":"Morizeyao\/GPT2-Chinese","26":"Morizeyao\/GPT2-Chinese","27":"Morizeyao\/GPT2-Chinese","28":"Morizeyao\/GPT2-Chinese","29":"Morizeyao\/GPT2-Chinese","30":"Morizeyao\/GPT2-Chinese","31":"Morizeyao\/GPT2-Chinese","32":"Morizeyao\/GPT2-Chinese"},"stargazers":{"0":4346,"1":4346,"2":4346,"3":4346,"4":4346,"5":4346,"6":4346,"7":4346,"8":4346,"9":4346,"10":4346,"11":4346,"12":4346,"13":4346,"14":4346,"15":4346,"16":4346,"17":4346,"18":4346,"19":4346,"20":4346,"21":4346,"22":4346,"23":4346,"24":4346,"25":4346,"26":4346,"27":4346,"28":4346,"29":4346,"30":4346,"31":4346,"32":4346}}