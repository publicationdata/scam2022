{"type":{"0":"W","1":"W","2":"W","3":"C","4":"W","5":"C","6":"W","7":"W","8":"R","9":"R","10":"R","11":"R","12":"R","13":"R","14":"R","15":"R","16":"R","17":"R","18":"R","19":"R","20":"R","21":"R","22":"R","23":"R","24":"R","25":"R","26":"R","27":"R","28":"R"},"module":{"0":"download_dataset","1":"download_models","2":"test_Fuji","3":"test_Fuji","4":"test_Sony","5":"test_Sony","6":"train_Fuji","7":"train_Sony","8":"train_Sony","9":"train_Sony","10":"train_Sony","11":"train_Sony","12":"train_Sony","13":"train_Sony","14":"train_Sony","15":"train_Sony","16":"train_Sony","17":"train_Sony","18":"train_Sony","19":"train_Sony","20":"train_Sony","21":"train_Sony","22":"train_Sony","23":"train_Sony","24":"train_Sony","25":"train_Sony","26":"train_Sony","27":"train_Sony","28":"train_Sony"},"obj":{"0":"download_file_from_google_drive","1":"download_file_from_google_drive","2":"network","3":"","4":"network","5":"","6":"network","7":"network","8":"","9":"","10":"","11":"","12":"","13":"","14":"","15":"","16":"","17":"","18":"","19":"","20":"","21":"","22":"","23":"","24":"","25":"","26":"","27":"","28":""},"lnum":{"0":4,"1":3,"2":34,"3":141,"4":41,"5":117,"6":37,"7":44,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1},"col":{"0":36,"1":36,"2":12,"3":4,"4":12,"5":4,"6":12,"7":12,"8":0,"9":0,"10":0,"11":0,"12":0,"13":0,"14":0,"15":0,"16":0,"17":0,"18":0,"19":0,"20":0,"21":0,"22":0,"23":0,"24":0,"25":0,"26":0,"27":0,"28":0},"filename":{"0":"download_dataset.py","1":"download_models.py","2":"test_Fuji.py","3":"test_Fuji.py","4":"test_Sony.py","5":"test_Sony.py","6":"train_Fuji.py","7":"train_Sony.py","8":"train_Sony.py","9":"train_Sony.py","10":"train_Sony.py","11":"train_Sony.py","12":"train_Sony.py","13":"train_Sony.py","14":"train_Sony.py","15":"train_Sony.py","16":"train_Sony.py","17":"train_Sony.py","18":"train_Sony.py","19":"train_Sony.py","20":"train_Sony.py","21":"train_Sony.py","22":"train_Sony.py","23":"train_Sony.py","24":"train_Sony.py","25":"train_Sony.py","26":"train_Sony.py","27":"train_Sony.py","28":"train_Sony.py"},"symbol":{"0":"redefined-builtin","1":"redefined-builtin","2":"redefined-builtin","3":"consider-using-enumerate","4":"redefined-builtin","5":"consider-using-enumerate","6":"redefined-builtin","7":"redefined-builtin","8":"duplicate-code","9":"duplicate-code","10":"duplicate-code","11":"duplicate-code","12":"duplicate-code","13":"duplicate-code","14":"duplicate-code","15":"duplicate-code","16":"duplicate-code","17":"duplicate-code","18":"duplicate-code","19":"duplicate-code","20":"duplicate-code","21":"duplicate-code","22":"duplicate-code","23":"duplicate-code","24":"duplicate-code","25":"duplicate-code","26":"duplicate-code","27":"duplicate-code","28":"duplicate-code"},"text":{"0":"Redefining built-in 'id'","1":"Redefining built-in 'id'","2":"Redefining built-in 'input'","3":"Consider using enumerate instead of iterating with range and len","4":"Redefining built-in 'input'","5":"Consider using enumerate instead of iterating with range and len","6":"Redefining built-in 'input'","7":"Redefining built-in 'input'","8":"Similar lines in 2 files\n==test_Fuji:18\n==train_Fuji:21\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):  # Unet\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 27, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 3)\n    return out\n\n\ndef pack_raw(raw):\n    # pack X-Trans image to 9 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 1024, 0) \/ (16383 - 1024)  # subtract the black level\n\n    img_shape = im.shape\n    H = (img_shape[0] \/\/ 6) * 6\n    W = (img_shape[1] \/\/ 6) * 6\n\n    out = np.zeros((H \/\/ 3, W \/\/ 3, 9))\n\n    # 0 R\n    out[0::2, 0::2, 0] = im[0:H:6, 0:W:6]\n    out[0::2, 1::2, 0] = im[0:H:6, 4:W:6]\n    out[1::2, 0::2, 0] = im[3:H:6, 1:W:6]\n    out[1::2, 1::2, 0] = im[3:H:6, 3:W:6]\n\n    # 1 G\n    out[0::2, 0::2, 1] = im[0:H:6, 2:W:6]\n    out[0::2, 1::2, 1] = im[0:H:6, 5:W:6]\n    out[1::2, 0::2, 1] = im[3:H:6, 2:W:6]\n    out[1::2, 1::2, 1] = im[3:H:6, 5:W:6]\n\n    # 1 B\n    out[0::2, 0::2, 2] = im[0:H:6, 1:W:6]\n    out[0::2, 1::2, 2] = im[0:H:6, 3:W:6]\n    out[1::2, 0::2, 2] = im[3:H:6, 0:W:6]\n    out[1::2, 1::2, 2] = im[3:H:6, 4:W:6]\n\n    # 4 R\n    out[0::2, 0::2, 3] = im[1:H:6, 2:W:6]\n    out[0::2, 1::2, 3] = im[2:H:6, 5:W:6]\n    out[1::2, 0::2, 3] = im[5:H:6, 2:W:6]\n    out[1::2, 1::2, 3] = im[4:H:6, 5:W:6]\n\n    # 5 B\n    out[0::2, 0::2, 4] = im[2:H:6, 2:W:6]\n    out[0::2, 1::2, 4] = im[1:H:6, 5:W:6]\n    out[1::2, 0::2, 4] = im[4:H:6, 2:W:6]\n    out[1::2, 1::2, 4] = im[5:H:6, 5:W:6]\n\n    out[:, :, 5] = im[1:H:3, 0:W:3]\n    out[:, :, 6] = im[1:H:3, 1:W:3]\n    out[:, :, 7] = im[2:H:3, 0:W:3]\n    out[:, :, 8] = im[2:H:3, 1:W:3]\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 9])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n","9":"Similar lines in 2 files\n==test_Sony:25\n==train_Sony:28\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n\n    conv10 = slim.conv2d(conv9, 12, [1, 1], rate=1, activation_fn=None, scope='g_conv10')\n    out = tf.depth_to_space(conv10, 2)\n    return out\n\n\ndef pack_raw(raw):\n    # pack Bayer image to 4 channels\n    im = raw.raw_image_visible.astype(np.float32)\n    im = np.maximum(im - 512, 0) \/ (16383 - 512)  # subtract the black level\n\n    im = np.expand_dims(im, axis=2)\n    img_shape = im.shape\n    H = img_shape[0]\n    W = img_shape[1]\n\n    out = np.concatenate((im[0:H:2, 0:W:2, :],\n                          im[0:H:2, 1:W:2, :],\n                          im[1:H:2, 1:W:2, :],\n                          im[1:H:2, 0:W:2, :]), axis=2)\n    return out\n\n\nsess = tf.Session()\nin_image = tf.placeholder(tf.float32, [None, None, None, 4])\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n","10":"Similar lines in 4 files\n==test_Fuji:18\n==test_Sony:25\n==train_Fuji:21\n==train_Sony:28\ndef lrelu(x):\n    return tf.maximum(x * 0.2, x)\n\n\ndef upsample_and_concat(x1, x2, output_channels, in_channels):\n    pool_size = 2\n    deconv_filter = tf.Variable(tf.truncated_normal([pool_size, pool_size, output_channels, in_channels], stddev=0.02))\n    deconv = tf.nn.conv2d_transpose(x1, deconv_filter, tf.shape(x2), strides=[1, pool_size, pool_size, 1])\n\n    deconv_output = tf.concat([deconv, x2], 3)\n    deconv_output.set_shape([None, None, None, output_channels * 2])\n\n    return deconv_output\n\n\ndef network(input):  # Unet\n    conv1 = slim.conv2d(input, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_1')\n    conv1 = slim.conv2d(conv1, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv1_2')\n    pool1 = slim.max_pool2d(conv1, [2, 2], padding='SAME')\n\n    conv2 = slim.conv2d(pool1, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_1')\n    conv2 = slim.conv2d(conv2, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv2_2')\n    pool2 = slim.max_pool2d(conv2, [2, 2], padding='SAME')\n\n    conv3 = slim.conv2d(pool2, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_1')\n    conv3 = slim.conv2d(conv3, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv3_2')\n    pool3 = slim.max_pool2d(conv3, [2, 2], padding='SAME')\n\n    conv4 = slim.conv2d(pool3, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_1')\n    conv4 = slim.conv2d(conv4, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv4_2')\n    pool4 = slim.max_pool2d(conv4, [2, 2], padding='SAME')\n\n    conv5 = slim.conv2d(pool4, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_1')\n    conv5 = slim.conv2d(conv5, 512, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv5_2')\n\n    up6 = upsample_and_concat(conv5, conv4, 256, 512)\n    conv6 = slim.conv2d(up6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_1')\n    conv6 = slim.conv2d(conv6, 256, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv6_2')\n\n    up7 = upsample_and_concat(conv6, conv3, 128, 256)\n    conv7 = slim.conv2d(up7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_1')\n    conv7 = slim.conv2d(conv7, 128, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv7_2')\n\n    up8 = upsample_and_concat(conv7, conv2, 64, 128)\n    conv8 = slim.conv2d(up8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_1')\n    conv8 = slim.conv2d(conv8, 64, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv8_2')\n\n    up9 = upsample_and_concat(conv8, conv1, 32, 64)\n    conv9 = slim.conv2d(up9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_1')\n    conv9 = slim.conv2d(conv9, 32, [3, 3], rate=1, activation_fn=lrelu, scope='g_conv9_2')\n","11":"Similar lines in 2 files\n==download_dataset:3\n==download_models:2\ndef download_file_from_google_drive(id, destination):\n    URL = \"https:\/\/docs.google.com\/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)\n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)\n\n\n","12":"Similar lines in 2 files\n==train_Fuji:150\n==train_Sony:126\ng_loss = np.zeros((5000, 1))\n\nallfolders = glob.glob(result_dir + '*0')\nlastepoch = 0\nfor folder in allfolders:\n    lastepoch = np.maximum(lastepoch, int(folder[-4:]))\n\nlearning_rate = 1e-4\nfor epoch in range(lastepoch, 4001):\n    if os.path.isdir(result_dir + '%04d' % epoch):\n        continue\n    cnt = 0\n    if epoch > 2000:\n        learning_rate = 1e-5\n\n    for ind in np.random.permutation(len(train_ids)):\n        # get the path from image id\n        train_id = train_ids[ind]","13":"Similar lines in 2 files\n==train_Fuji:127\n==train_Sony:103\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nG_loss = tf.reduce_mean(tf.abs(out_image - gt_image))\n\nt_vars = tf.trainable_variables()\nlr = tf.placeholder(tf.float32)\nG_opt = tf.train.AdamOptimizer(learning_rate=lr).minimize(G_loss)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\n# Raw data takes long time to load. Keep them in memory after loaded.\ngt_images = [None] * 6000","14":"Similar lines in 2 files\n==train_Fuji:199\n==train_Sony:175\n        if np.random.randint(2, size=1)[0] == 1:  # random flip\n            input_patch = np.flip(input_patch, axis=1)\n            gt_patch = np.flip(gt_patch, axis=1)\n        if np.random.randint(2, size=1)[0] == 1:\n            input_patch = np.flip(input_patch, axis=2)\n            gt_patch = np.flip(gt_patch, axis=2)\n        if np.random.randint(2, size=1)[0] == 1:  # random transpose\n            input_patch = np.transpose(input_patch, (0, 2, 1, 3))\n            gt_patch = np.transpose(gt_patch, (0, 2, 1, 3))\n\n        input_patch = np.minimum(input_patch, 1.0)\n\n        _, G_current, output = sess.run([G_opt, G_loss, out_image],\n                                        feed_dict={in_image: input_patch, gt_image: gt_patch, lr: learning_rate})\n        output = np.minimum(np.maximum(output, 0), 1)\n        g_loss[ind] = G_current\n","15":"Similar lines in 2 files\n==test_Fuji:124\n==test_Sony:100\ngt_image = tf.placeholder(tf.float32, [None, None, None, 3])\nout_image = network(in_image)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n\nif not os.path.isdir(result_dir + 'final\/'):\n    os.makedirs(result_dir + 'final\/')\n\nfor test_id in test_ids:\n    # test the first image in each sequence","16":"Similar lines in 2 files\n==test_Fuji:153\n==test_Sony:130\n        im = raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        # scale_full = np.expand_dims(np.float32(im\/65535.0),axis = 0)*ratio #scale the low-light image using the same ratio\n        scale_full = np.expand_dims(np.float32(im \/ 65535.0), axis=0)\n\n        gt_raw = rawpy.imread(gt_path)\n        im = gt_raw.postprocess(use_camera_wb=True, half_size=False, no_auto_bright=True, output_bps=16)\n        gt_full = np.expand_dims(np.float32(im \/ 65535.0), axis=0)\n\n        input_full = np.minimum(input_full, 1.0)\n\n        output = sess.run(out_image, feed_dict={in_image: input_full})\n        output = np.minimum(np.maximum(output, 0), 1)\n","17":"Similar lines in 2 files\n==train_Fuji:218\n==train_Sony:194\n        if epoch % save_freq == 0:\n            if not os.path.isdir(result_dir + '%04d' % epoch):\n                os.makedirs(result_dir + '%04d' % epoch)\n\n            temp = np.concatenate((gt_patch[0, :, :, :], output[0, :, :, :]), axis=1)\n            scipy.misc.toimage(temp * 255, high=255, low=0, cmin=0, cmax=255).save(\n                result_dir + '%04d\/%05d_00_train_%d.jpg' % (epoch, train_id, ratio))\n\n    saver.save(sess, checkpoint_dir + 'model.ckpt')","18":"Similar lines in 2 files\n==train_Fuji:173\n==train_Sony:149\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure \/ in_exposure, 300)\n\n        st = time.time()\n        cnt += 1\n","19":"Similar lines in 2 files\n==test_Fuji:171\n==test_Sony:146\n        scale_full = scale_full * np.mean(gt_full) \/ np.mean(\n            scale_full)  # scale the low-light image to the same mean of the groundtruth\n\n        scipy.misc.toimage(output * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final\/%5d_00_%d_out.png' % (test_id, ratio))\n        scipy.misc.toimage(scale_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final\/%5d_00_%d_scale.png' % (test_id, ratio))\n        scipy.misc.toimage(gt_full * 255, high=255, low=0, cmin=0, cmax=255).save(\n            result_dir + 'final\/%5d_00_%d_gt.png' % (test_id, ratio))","20":"Similar lines in 2 files\n==train_Fuji:0\n==train_Sony:2\nfrom __future__ import division\nimport os, time, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n","21":"Similar lines in 2 files\n==test_Sony:4\n==train_Sony:4\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = '.\/dataset\/Sony\/short\/'\ngt_dir = '.\/dataset\/Sony\/long\/'","22":"Similar lines in 2 files\n==test_Fuji:2\n==train_Fuji:2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n\ninput_dir = '.\/dataset\/Fuji\/short\/'\ngt_dir = '.\/dataset\/Fuji\/long\/'","23":"Similar lines in 2 files\n==test_Fuji:145\n==test_Sony:121\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure \/ in_exposure, 300)\n\n        raw = rawpy.imread(in_path)\n        input_full = np.expand_dims(pack_raw(raw), axis=0) * ratio","24":"Similar lines in 2 files\n==test_Fuji:0\n==test_Sony:2\nfrom __future__ import division\nimport os, scipy.io\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n","25":"Similar lines in 4 files\n==test_Fuji:127\n==test_Sony:103\n==train_Fuji:136\n==train_Sony:112\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\nckpt = tf.train.get_checkpoint_state(checkpoint_dir)\nif ckpt:\n    print('loaded ' + ckpt.model_checkpoint_path)\n    saver.restore(sess, ckpt.model_checkpoint_path)\n","26":"Similar lines in 2 files\n==test_Sony:4\n==train_Fuji:2\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n","27":"Similar lines in 2 files\n==test_Fuji:2\n==train_Sony:4\nimport tensorflow as tf\nimport tensorflow.contrib.slim as slim\nimport numpy as np\nimport rawpy\nimport glob\n","28":"Similar lines in 4 files\n==test_Fuji:145\n==test_Sony:121\n==train_Fuji:173\n==train_Sony:149\n        gt_path = gt_files[0]\n        gt_fn = os.path.basename(gt_path)\n        in_exposure = float(in_fn[9:-5])\n        gt_exposure = float(gt_fn[9:-5])\n        ratio = min(gt_exposure \/ in_exposure, 300)\n"},"number":{"0":"W0622","1":"W0622","2":"W0622","3":"C0200","4":"W0622","5":"C0200","6":"W0622","7":"W0622","8":"R0801","9":"R0801","10":"R0801","11":"R0801","12":"R0801","13":"R0801","14":"R0801","15":"R0801","16":"R0801","17":"R0801","18":"R0801","19":"R0801","20":"R0801","21":"R0801","22":"R0801","23":"R0801","24":"R0801","25":"R0801","26":"R0801","27":"R0801","28":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint","17":"pylint","18":"pylint","19":"pylint","20":"pylint","21":"pylint","22":"pylint","23":"pylint","24":"pylint","25":"pylint","26":"pylint","27":"pylint","28":"pylint"},"lines_amount":{"0":43,"1":42,"2":181,"3":181,"4":156,"5":156,"6":228,"7":204,"8":204,"9":204,"10":204,"11":204,"12":204,"13":204,"14":204,"15":204,"16":204,"17":204,"18":204,"19":204,"20":204,"21":204,"22":204,"23":204,"24":204,"25":204,"26":204,"27":204,"28":204},"commit":{"0":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","1":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","2":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","3":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","4":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","5":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","6":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","7":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","8":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","9":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","10":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","11":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","12":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","13":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","14":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","15":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","16":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","17":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","18":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","19":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","20":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","21":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","22":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","23":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","24":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","25":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","26":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","27":"7a8c4114b69b8850e3e7a87ff885c217381a71bc","28":"7a8c4114b69b8850e3e7a87ff885c217381a71bc"},"repo":{"0":"cchen156\/Learning-to-See-in-the-Dark","1":"cchen156\/Learning-to-See-in-the-Dark","2":"cchen156\/Learning-to-See-in-the-Dark","3":"cchen156\/Learning-to-See-in-the-Dark","4":"cchen156\/Learning-to-See-in-the-Dark","5":"cchen156\/Learning-to-See-in-the-Dark","6":"cchen156\/Learning-to-See-in-the-Dark","7":"cchen156\/Learning-to-See-in-the-Dark","8":"cchen156\/Learning-to-See-in-the-Dark","9":"cchen156\/Learning-to-See-in-the-Dark","10":"cchen156\/Learning-to-See-in-the-Dark","11":"cchen156\/Learning-to-See-in-the-Dark","12":"cchen156\/Learning-to-See-in-the-Dark","13":"cchen156\/Learning-to-See-in-the-Dark","14":"cchen156\/Learning-to-See-in-the-Dark","15":"cchen156\/Learning-to-See-in-the-Dark","16":"cchen156\/Learning-to-See-in-the-Dark","17":"cchen156\/Learning-to-See-in-the-Dark","18":"cchen156\/Learning-to-See-in-the-Dark","19":"cchen156\/Learning-to-See-in-the-Dark","20":"cchen156\/Learning-to-See-in-the-Dark","21":"cchen156\/Learning-to-See-in-the-Dark","22":"cchen156\/Learning-to-See-in-the-Dark","23":"cchen156\/Learning-to-See-in-the-Dark","24":"cchen156\/Learning-to-See-in-the-Dark","25":"cchen156\/Learning-to-See-in-the-Dark","26":"cchen156\/Learning-to-See-in-the-Dark","27":"cchen156\/Learning-to-See-in-the-Dark","28":"cchen156\/Learning-to-See-in-the-Dark"},"stargazers":{"0":5103,"1":5103,"2":5103,"3":5103,"4":5103,"5":5103,"6":5103,"7":5103,"8":5103,"9":5103,"10":5103,"11":5103,"12":5103,"13":5103,"14":5103,"15":5103,"16":5103,"17":5103,"18":5103,"19":5103,"20":5103,"21":5103,"22":5103,"23":5103,"24":5103,"25":5103,"26":5103,"27":5103,"28":5103}}