{"type":{"0":"C","1":"W","2":"C","3":"R","4":"R","5":"R","6":"R","7":"R","8":"R","9":"R","10":"R","11":"R","12":"R","13":"R","14":"R","15":"R","16":"R","17":"R","18":"R","19":"R"},"module":{"0":"u2net_portrait_demo","1":"u2net_portrait_demo","2":"u2net_portrait_demo","3":"u2net_train","4":"u2net_train","5":"u2net_train","6":"u2net_train","7":"u2net_train","8":"u2net_train","9":"u2net_train","10":"u2net_train","11":"u2net_train","12":"u2net_train","13":"u2net_train","14":"u2net_train","15":"u2net_train","16":"u2net_train","17":"u2net_train","18":"u2net_train","19":"u2net_train"},"obj":{"0":"detect_single_face","1":"inference","2":"main","3":"","4":"","5":"","6":"","7":"","8":"","9":"","10":"","11":"","12":"","13":"","14":"","15":"","16":"","17":"","18":"","19":""},"lnum":{"0":22,"1":99,"2":160,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1},"col":{"0":4,"1":18,"2":4,"3":0,"4":0,"5":0,"6":0,"7":0,"8":0,"9":0,"10":0,"11":0,"12":0,"13":0,"14":0,"15":0,"16":0,"17":0,"18":0,"19":0},"filename":{"0":"u2net_portrait_demo.py","1":"u2net_portrait_demo.py","2":"u2net_portrait_demo.py","3":"u2net_train.py","4":"u2net_train.py","5":"u2net_train.py","6":"u2net_train.py","7":"u2net_train.py","8":"u2net_train.py","9":"u2net_train.py","10":"u2net_train.py","11":"u2net_train.py","12":"u2net_train.py","13":"u2net_train.py","14":"u2net_train.py","15":"u2net_train.py","16":"u2net_train.py","17":"u2net_train.py","18":"u2net_train.py","19":"u2net_train.py"},"symbol":{"0":"consider-using-enumerate","1":"redefined-builtin","2":"consider-using-enumerate","3":"duplicate-code","4":"duplicate-code","5":"duplicate-code","6":"duplicate-code","7":"duplicate-code","8":"duplicate-code","9":"duplicate-code","10":"duplicate-code","11":"duplicate-code","12":"duplicate-code","13":"duplicate-code","14":"duplicate-code","15":"duplicate-code","16":"duplicate-code","17":"duplicate-code","18":"duplicate-code","19":"duplicate-code"},"text":{"0":"Consider using enumerate instead of iterating with range and len","1":"Redefining built-in 'input'","2":"Consider using enumerate instead of iterating with range and len","3":"Similar lines in 2 files\n==u2net_portrait_test:0\n==u2net_test:0\nimport os\nfrom skimage import io, transform\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms#, utils\n# import torch.optim as optim\n\nimport numpy as np\nfrom PIL import Image\nimport glob\n\nfrom data_loader import RescaleT\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB\nfrom model import U2NETP # small version u2net 4.7 MB\n\n# normalize the predicted SOD probability map\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)\/(ma-mi)\n\n    return dn\n\ndef save_output(image_name,pred,d_dir):\n\n    predict = pred\n    predict = predict.squeeze()\n    predict_np = predict.cpu().data.numpy()\n\n    im = Image.fromarray(predict_np*255).convert('RGB')\n    img_name = image_name.split(os.sep)[-1]\n    image = io.imread(image_name)\n    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n\n    pb_np = np.array(imo)\n\n    aaa = img_name.split(\".\")\n    bbb = aaa[0:-1]\n    imidx = bbb[0]\n    for i in range(1,len(bbb)):\n        imidx = imidx + \".\" + bbb[i]\n","4":"Similar lines in 2 files\n==u2net_portrait_composite:85\n==u2net_portrait_test:61\n    if(not os.path.exists(prediction_dir)):\n        os.mkdir(prediction_dir)\n\n    model_dir = '.\/saved_models\/u2net_portrait\/u2net_portrait.pth'\n\n    img_name_list = glob.glob(image_dir+'\/*')\n    print(\"Number of images: \", len(img_name_list))\n\n    # --------- 2. dataloader ---------\n    #1. dataloader\n    test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n                                        lbl_name_list = [],\n                                        transform=transforms.Compose([RescaleT(512),\n                                                                      ToTensorLab(flag=0)])\n                                        )\n    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n                                        batch_size=1,\n                                        shuffle=False,\n                                        num_workers=1)\n\n    # --------- 3. model define ---------\n\n    print(\"...load U2NET---173.6 MB\")\n    net = U2NET(3,1)\n\n    net.load_state_dict(torch.load(model_dir))\n    if torch.cuda.is_available():\n        net.cuda()\n    net.eval()\n\n    # --------- 4. inference for each image ---------\n    for i_test, data_test in enumerate(test_salobj_dataloader):\n\n        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n\n        inputs_test = data_test['image']\n        inputs_test = inputs_test.type(torch.FloatTensor)\n\n        if torch.cuda.is_available():\n            inputs_test = Variable(inputs_test.cuda())\n        else:\n            inputs_test = Variable(inputs_test)\n\n        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n\n        # normalization\n        pred = 1.0 - d1[:,0,:,:]\n        pred = normPRED(pred)\n\n        # save results to test_results folder","5":"Similar lines in 2 files\n==u2net_human_seg_test:82\n==u2net_test:87\n    if torch.cuda.is_available():\n        net.load_state_dict(torch.load(model_dir))\n        net.cuda()\n    else:\n        net.load_state_dict(torch.load(model_dir, map_location='cpu'))\n    net.eval()\n\n    # --------- 4. inference for each image ---------\n    for i_test, data_test in enumerate(test_salobj_dataloader):\n\n        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n\n        inputs_test = data_test['image']\n        inputs_test = inputs_test.type(torch.FloatTensor)\n\n        if torch.cuda.is_available():\n            inputs_test = Variable(inputs_test.cuda())\n        else:\n            inputs_test = Variable(inputs_test)\n\n        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n\n        # normalization\n        pred = d1[:,0,:,:]\n        pred = normPRED(pred)\n\n        # save results to test_results folder\n        if not os.path.exists(prediction_dir):\n            os.makedirs(prediction_dir, exist_ok=True)\n        save_output(img_name_list[i_test],pred,prediction_dir)\n\n        del d1,d2,d3,d4,d5,d6,d7\n\nif __name__ == \"__main__\":\n    main()","6":"Similar lines in 2 files\n==u2net_human_seg_test:23\n==u2net_test:24\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)\/(ma-mi)\n\n    return dn\n\ndef save_output(image_name,pred,d_dir):\n\n    predict = pred\n    predict = predict.squeeze()\n    predict_np = predict.cpu().data.numpy()\n\n    im = Image.fromarray(predict_np*255).convert('RGB')\n    img_name = image_name.split(os.sep)[-1]\n    image = io.imread(image_name)\n    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n\n    pb_np = np.array(imo)\n\n    aaa = img_name.split(\".\")\n    bbb = aaa[0:-1]\n    imidx = bbb[0]\n    for i in range(1,len(bbb)):\n        imidx = imidx + \".\" + bbb[i]\n\n    imo.save(d_dir+imidx+'.png')\n\ndef main():\n\n    # --------- 1. get image path and name ---------\n    model_name='u2net'#u2netp\n\n","7":"Similar lines in 2 files\n==u2net_human_seg_test:23\n==u2net_portrait_test:24\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)\/(ma-mi)\n\n    return dn\n\ndef save_output(image_name,pred,d_dir):\n\n    predict = pred\n    predict = predict.squeeze()\n    predict_np = predict.cpu().data.numpy()\n\n    im = Image.fromarray(predict_np*255).convert('RGB')\n    img_name = image_name.split(os.sep)[-1]\n    image = io.imread(image_name)\n    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n\n    pb_np = np.array(imo)\n\n    aaa = img_name.split(\".\")\n    bbb = aaa[0:-1]\n    imidx = bbb[0]\n    for i in range(1,len(bbb)):\n        imidx = imidx + \".\" + bbb[i]\n","8":"Similar lines in 3 files\n==u2net_portrait_composite:3\n==u2net_portrait_test:2\n==u2net_test:2\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms#, utils\n# import torch.optim as optim\n\nimport numpy as np\nfrom PIL import Image\nimport glob\n\nfrom data_loader import RescaleT\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB\nfrom model import U2NETP # small version u2net 4.7 MB\n","9":"Similar lines in 3 files\n==u2net_human_seg_test:0\n==u2net_portrait_test:0\n==u2net_test:0\nimport os\nfrom skimage import io, transform\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms#, utils\n# import torch.optim as optim\n\nimport numpy as np\nfrom PIL import Image\nimport glob\n\nfrom data_loader import RescaleT\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB","10":"Similar lines in 2 files\n==u2net_human_seg_test:62\n==u2net_test:64\n    img_name_list = glob.glob(image_dir + os.sep + '*')\n    print(img_name_list)\n\n    # --------- 2. dataloader ---------\n    #1. dataloader\n    test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n                                        lbl_name_list = [],\n                                        transform=transforms.Compose([RescaleT(320),\n                                                                      ToTensorLab(flag=0)])\n                                        )\n    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n                                        batch_size=1,\n                                        shuffle=False,\n                                        num_workers=1)\n\n    # --------- 3. model define ---------\n    if(model_name=='u2net'):\n        print(\"...load U2NET---173.6 MB\")\n        net = U2NET(3,1)","11":"Similar lines in 2 files\n==u2net_human_seg_test:2\n==u2net_portrait_composite:3\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms#, utils\n# import torch.optim as optim\n\nimport numpy as np\nfrom PIL import Image\nimport glob\n\nfrom data_loader import RescaleT\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB","12":"Similar lines in 4 files\n==u2net_human_seg_test:87\n==u2net_portrait_composite:113\n==u2net_portrait_test:89\n==u2net_test:92\n    net.eval()\n\n    # --------- 4. inference for each image ---------\n    for i_test, data_test in enumerate(test_salobj_dataloader):\n\n        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n\n        inputs_test = data_test['image']\n        inputs_test = inputs_test.type(torch.FloatTensor)\n\n        if torch.cuda.is_available():\n            inputs_test = Variable(inputs_test.cuda())\n        else:\n            inputs_test = Variable(inputs_test)\n\n        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n\n        # normalization","13":"Similar lines in 3 files\n==u2net_portrait_test:16\n==u2net_test:16\n==u2net_train:19\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB\nfrom model import U2NETP # small version u2net 4.7 MB\n\n# normalize the predicted SOD probability map","14":"Similar lines in 4 files\n==u2net_human_seg_test:70\n==u2net_portrait_composite:98\n==u2net_portrait_test:74\n==u2net_test:72\n                                                                      ToTensorLab(flag=0)])\n                                        )\n    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n                                        batch_size=1,\n                                        shuffle=False,\n                                        num_workers=1)\n\n    # --------- 3. model define ---------","15":"Similar lines in 5 files\n==u2net_human_seg_test:23\n==u2net_portrait_composite:27\n==u2net_portrait_demo:90\n==u2net_portrait_test:24\n==u2net_test:24\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)\/(ma-mi)\n\n    return dn\n","16":"Similar lines in 2 files\n==u2net_portrait_composite:17\n==u2net_train:19\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB\nfrom model import U2NETP # small version u2net 4.7 MB\n","17":"Similar lines in 4 files\n==u2net_human_seg_test:44\n==u2net_portrait_test:45\n==u2net_test:45\n==u2net_train:71\n    aaa = img_name.split(\".\")\n    bbb = aaa[0:-1]\n    imidx = bbb[0]\n    for i in range(1,len(bbb)):\n        imidx = imidx + \".\" + bbb[i]\n","18":"Similar lines in 5 files\n==u2net_human_seg_test:2\n==u2net_portrait_composite:3\n==u2net_portrait_test:2\n==u2net_test:2\n==u2net_train:1\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F","19":"Similar lines in 5 files\n==u2net_human_seg_test:44\n==u2net_portrait_composite:62\n==u2net_portrait_test:45\n==u2net_test:45\n==u2net_train:71\n\taaa = img_name.split(\".\")\n\tbbb = aaa[0:-1]\n\timidx = bbb[0]\n\tfor i in range(1,len(bbb)):\n\t\timidx = imidx + \".\" + bbb[i]"},"number":{"0":"C0200","1":"W0622","2":"C0200","3":"R0801","4":"R0801","5":"R0801","6":"R0801","7":"R0801","8":"R0801","9":"R0801","10":"R0801","11":"R0801","12":"R0801","13":"R0801","14":"R0801","15":"R0801","16":"R0801","17":"R0801","18":"R0801","19":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint","17":"pylint","18":"pylint","19":"pylint"},"lines_amount":{"0":176,"1":176,"2":176,"3":165,"4":165,"5":165,"6":165,"7":165,"8":165,"9":165,"10":165,"11":165,"12":165,"13":165,"14":165,"15":165,"16":165,"17":165,"18":165,"19":165},"commit":{"0":"d3da56874265b1853beff6ff8ddae891c1ae262b","1":"d3da56874265b1853beff6ff8ddae891c1ae262b","2":"d3da56874265b1853beff6ff8ddae891c1ae262b","3":"d3da56874265b1853beff6ff8ddae891c1ae262b","4":"d3da56874265b1853beff6ff8ddae891c1ae262b","5":"d3da56874265b1853beff6ff8ddae891c1ae262b","6":"d3da56874265b1853beff6ff8ddae891c1ae262b","7":"d3da56874265b1853beff6ff8ddae891c1ae262b","8":"d3da56874265b1853beff6ff8ddae891c1ae262b","9":"d3da56874265b1853beff6ff8ddae891c1ae262b","10":"d3da56874265b1853beff6ff8ddae891c1ae262b","11":"d3da56874265b1853beff6ff8ddae891c1ae262b","12":"d3da56874265b1853beff6ff8ddae891c1ae262b","13":"d3da56874265b1853beff6ff8ddae891c1ae262b","14":"d3da56874265b1853beff6ff8ddae891c1ae262b","15":"d3da56874265b1853beff6ff8ddae891c1ae262b","16":"d3da56874265b1853beff6ff8ddae891c1ae262b","17":"d3da56874265b1853beff6ff8ddae891c1ae262b","18":"d3da56874265b1853beff6ff8ddae891c1ae262b","19":"d3da56874265b1853beff6ff8ddae891c1ae262b"},"repo":{"0":"xuebinqin\/U-2-Net","1":"xuebinqin\/U-2-Net","2":"xuebinqin\/U-2-Net","3":"xuebinqin\/U-2-Net","4":"xuebinqin\/U-2-Net","5":"xuebinqin\/U-2-Net","6":"xuebinqin\/U-2-Net","7":"xuebinqin\/U-2-Net","8":"xuebinqin\/U-2-Net","9":"xuebinqin\/U-2-Net","10":"xuebinqin\/U-2-Net","11":"xuebinqin\/U-2-Net","12":"xuebinqin\/U-2-Net","13":"xuebinqin\/U-2-Net","14":"xuebinqin\/U-2-Net","15":"xuebinqin\/U-2-Net","16":"xuebinqin\/U-2-Net","17":"xuebinqin\/U-2-Net","18":"xuebinqin\/U-2-Net","19":"xuebinqin\/U-2-Net"},"stargazers":{"0":4929,"1":4929,"2":4929,"3":4929,"4":4929,"5":4929,"6":4929,"7":4929,"8":4929,"9":4929,"10":4929,"11":4929,"12":4929,"13":4929,"14":4929,"15":4929,"16":4929,"17":4929,"18":4929,"19":4929}}