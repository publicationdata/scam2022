{"type":{"0":"R","1":"R","2":"R","3":"R","4":"R","5":"R","6":"R","7":"R","8":"R","9":"R","10":"R","11":"R","12":"R","13":"R","14":"R","15":"R","16":"R","17":"R","18":"R","19":"R"},"module":{"0":"train_refine","1":"train_refine","2":"train_refine","3":"train_refine","4":"train_refine","5":"train_refine","6":"train_refine","7":"train_refine","8":"train_refine","9":"train_refine","10":"train_refine","11":"train_refine","12":"train_refine","13":"train_refine","14":"train_refine","15":"train_refine","16":"train_refine","17":"train_refine","18":"train_refine","19":"train_refine"},"obj":{"0":"","1":"","2":"","3":"","4":"","5":"","6":"","7":"","8":"","9":"","10":"","11":"","12":"","13":"","14":"","15":"","16":"","17":"","18":"","19":""},"lnum":{"0":1,"1":1,"2":1,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1},"col":{"0":0,"1":0,"2":0,"3":0,"4":0,"5":0,"6":0,"7":0,"8":0,"9":0,"10":0,"11":0,"12":0,"13":0,"14":0,"15":0,"16":0,"17":0,"18":0,"19":0},"filename":{"0":"train_refine.py","1":"train_refine.py","2":"train_refine.py","3":"train_refine.py","4":"train_refine.py","5":"train_refine.py","6":"train_refine.py","7":"train_refine.py","8":"train_refine.py","9":"train_refine.py","10":"train_refine.py","11":"train_refine.py","12":"train_refine.py","13":"train_refine.py","14":"train_refine.py","15":"train_refine.py","16":"train_refine.py","17":"train_refine.py","18":"train_refine.py","19":"train_refine.py"},"symbol":{"0":"duplicate-code","1":"duplicate-code","2":"duplicate-code","3":"duplicate-code","4":"duplicate-code","5":"duplicate-code","6":"duplicate-code","7":"duplicate-code","8":"duplicate-code","9":"duplicate-code","10":"duplicate-code","11":"duplicate-code","12":"duplicate-code","13":"duplicate-code","14":"duplicate-code","15":"duplicate-code","16":"duplicate-code","17":"duplicate-code","18":"duplicate-code","19":"duplicate-code"},"text":{"0":"Similar lines in 2 files\n==train_base:147\n==train_refine:174\n            true_pha, true_fgr, true_bgr = random_crop(true_pha, true_fgr, true_bgr)\n\n            true_src = true_bgr.clone()\n\n            # Augment with shadow\n            aug_shadow_idx = torch.rand(len(true_src)) < 0.3\n            if aug_shadow_idx.any():\n                aug_shadow = true_pha[aug_shadow_idx].mul(0.3 * random.random())\n                aug_shadow = T.RandomAffine(degrees=(-5, 5), translate=(0.2, 0.2), scale=(0.5, 1.5), shear=(-5, 5))(aug_shadow)\n                aug_shadow = kornia.filters.box_blur(aug_shadow, (random.choice(range(20, 40)),) * 2)\n                true_src[aug_shadow_idx] = true_src[aug_shadow_idx].sub_(aug_shadow).clamp_(0, 1)\n                del aug_shadow\n            del aug_shadow_idx\n\n            # Composite foreground onto source\n            true_src = true_fgr * true_pha + true_src * (1 - true_pha)\n\n            # Augment with noise\n            aug_noise_idx = torch.rand(len(true_src)) < 0.4\n            if aug_noise_idx.any():\n                true_src[aug_noise_idx] = true_src[aug_noise_idx].add_(torch.randn_like(true_src[aug_noise_idx]).mul_(0.03 * random.random())).clamp_(0, 1)\n                true_bgr[aug_noise_idx] = true_bgr[aug_noise_idx].add_(torch.randn_like(true_bgr[aug_noise_idx]).mul_(0.03 * random.random())).clamp_(0, 1)\n            del aug_noise_idx\n\n            # Augment background with jitter\n            aug_jitter_idx = torch.rand(len(true_src)) < 0.8\n            if aug_jitter_idx.any():\n                true_bgr[aug_jitter_idx] = kornia.augmentation.ColorJitter(0.18, 0.18, 0.18, 0.1)(true_bgr[aug_jitter_idx])\n            del aug_jitter_idx\n\n            # Augment background with affine\n            aug_affine_idx = torch.rand(len(true_bgr)) < 0.3\n            if aug_affine_idx.any():\n                true_bgr[aug_affine_idx] = T.RandomAffine(degrees=(-1, 1), translate=(0.01, 0.01))(true_bgr[aug_affine_idx])\n            del aug_affine_idx\n\n            with autocast():","1":"Similar lines in 2 files\n==inference_images:76\n==inference_video:114\ndevice = torch.device(args.device)\n\n# Load model\nif args.model_type == 'mattingbase':\n    model = MattingBase(args.model_backbone)\nif args.model_type == 'mattingrefine':\n    model = MattingRefine(\n        args.model_backbone,\n        args.model_backbone_scale,\n        args.model_refine_mode,\n        args.model_refine_sample_pixels,\n        args.model_refine_threshold,\n        args.model_refine_kernel_size)\n\nmodel = model.to(device).eval()\nmodel.load_state_dict(torch.load(args.model_checkpoint, map_location=device), strict=False)\n\n\n# Load video and background","2":"Similar lines in 2 files\n==train_base:234\n==train_refine:273\n        results.append(img)\n    return results\n\n\ndef valid(model, dataloader, writer, step):\n    model.eval()\n    loss_total = 0\n    loss_count = 0\n    with torch.no_grad():\n        for (true_pha, true_fgr), true_bgr in dataloader:\n            batch_size = true_pha.size(0)\n\n            true_pha = true_pha.cuda(non_blocking=True)\n            true_fgr = true_fgr.cuda(non_blocking=True)\n            true_bgr = true_bgr.cuda(non_blocking=True)\n            true_src = true_pha * true_fgr + (1 - true_pha) * true_bgr\n","3":"Similar lines in 3 files\n==inference_images:76\n==inference_speed_test:79\n==inference_video:114\ndevice = torch.device(args.device)\n\n# Load model\nif args.model_type == 'mattingbase':\n    model = MattingBase(args.model_backbone)\nif args.model_type == 'mattingrefine':\n    model = MattingRefine(\n        args.model_backbone,\n        args.model_backbone_scale,\n        args.model_refine_mode,\n        args.model_refine_sample_pixels,\n        args.model_refine_threshold,","4":"Similar lines in 2 files\n==train_base:253\n==train_refine:292\n            loss_total += loss.cpu().item() * batch_size\n            loss_count += batch_size\n\n    writer.add_scalar('valid_loss', loss_total \/ loss_count, step)\n    model.train()\n\n\n# --------------- Start ---------------\n\n\nif __name__ == '__main__':","5":"Similar lines in 2 files\n==inference_images:64\n==inference_video:68\nargs = parser.parse_args()\n\n\nassert 'err' not in args.output_types or args.model_type in ['mattingbase', 'mattingrefine'], \\\n    'Only mattingbase and mattingrefine support err output'\nassert 'ref' not in args.output_types or args.model_type in ['mattingrefine'], \\\n    'Only mattingrefine support ref output'\n\n# --------------- Utils ---------------\n\n","6":"Similar lines in 2 files\n==inference_images:20\n==inference_video:23\nimport torch\nimport os\nimport shutil\n\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms as T\nfrom torchvision.transforms.functional import to_pil_image\nfrom threading import Thread\nfrom tqdm import tqdm","7":"Similar lines in 2 files\n==train_base:135\n==train_refine:162\n        if not os.path.exists(f'checkpoint\/{args.model_name}'):\n            os.makedirs(f'checkpoint\/{args.model_name}')\n        writer = SummaryWriter(f'log\/{args.model_name}')\n\n    # Run loop\n    for epoch in range(args.epoch_start, args.epoch_end):\n        for i, ((true_pha, true_fgr), true_bgr) in enumerate(tqdm(dataloader_train)):\n            step = epoch * len(dataloader_train) + i\n","8":"Similar lines in 2 files\n==train_base:27\n==train_refine:30\nfrom torch.optim import Adam\nfrom torchvision.utils import make_grid\nfrom tqdm import tqdm\nfrom torchvision import transforms as T\nfrom PIL import Image\n\nfrom data_path import DATA_PATH\nfrom dataset import ImagesDataset, ZipDataset, VideoDataset, SampleDataset\nfrom dataset import augmentation as A","9":"Similar lines in 2 files\n==inference_images:43\n==inference_video:47\nparser.add_argument('--model-type', type=str, required=True, choices=['mattingbase', 'mattingrefine'])\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-checkpoint', type=str, required=True)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)\nparser.add_argument('--model-refine-threshold', type=float, default=0.7)\nparser.add_argument('--model-refine-kernel-size', type=int, default=3)\n","10":"Similar lines in 4 files\n==inference_images:79\n==inference_speed_test:82\n==inference_video:117\n==inference_webcam:126\nif args.model_type == 'mattingbase':\n    model = MattingBase(args.model_backbone)\nif args.model_type == 'mattingrefine':\n    model = MattingRefine(\n        args.model_backbone,\n        args.model_backbone_scale,\n        args.model_refine_mode,\n        args.model_refine_sample_pixels,","11":"Similar lines in 2 files\n==train_base:192\n==train_refine:220\n                if (i + 1) % args.log_train_loss_interval == 0:\n                    writer.add_scalar('loss', loss, step)\n\n                if (i + 1) % args.log_train_images_interval == 0:\n                    writer.add_image('train_pred_pha', make_grid(pred_pha, nrow=5), step)\n                    writer.add_image('train_pred_fgr', make_grid(pred_fgr, nrow=5), step)\n                    writer.add_image('train_pred_com', make_grid(pred_fgr * pred_pha, nrow=5), step)","12":"Similar lines in 2 files\n==train_base:86\n==train_refine:101\n            T.RandomHorizontalFlip(),\n            A.RandomBoxBlur(0.1, 5),\n            A.RandomSharpen(0.1),\n            T.ColorJitter(0.15, 0.15, 0.15, 0.05),\n            T.ToTensor()\n        ])),\n    ])","13":"Similar lines in 2 files\n==train_base:78\n==train_refine:93\n            A.PairRandomHorizontalFlip(),\n            A.PairRandomBoxBlur(0.1, 5),\n            A.PairRandomSharpen(0.1),\n            A.PairApplyOnlyAtIndices([1], T.ColorJitter(0.15, 0.15, 0.15, 0.05)),\n            A.PairApply(T.ToTensor())\n        ]), assert_equal_length=True),\n        ImagesDataset(DATA_PATH['backgrounds']['train'], mode='RGB', transforms=T.Compose([","14":"Similar lines in 2 files\n==train_base:16\n==train_refine:17\nimport argparse\nimport kornia\nimport torch\nimport os\nimport random\n\nfrom torch import nn","15":"Similar lines in 3 files\n==inference_images:43\n==inference_video:47\n==inference_webcam:38\nparser.add_argument('--model-type', type=str, required=True, choices=['mattingbase', 'mattingrefine'])\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-checkpoint', type=str, required=True)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)\nparser.add_argument('--model-refine-threshold', type=float, default=0.7)","16":"Similar lines in 2 files\n==inference_images:125\n==inference_video:188\n        src = src.to(device, non_blocking=True)\n        bgr = bgr.to(device, non_blocking=True)\n\n        if args.model_type == 'mattingbase':\n            pha, fgr, err, _ = model(src, bgr)\n        elif args.model_type == 'mattingrefine':\n            pha, fgr, _, _, err, ref = model(src, bgr)","17":"Similar lines in 4 files\n==export_onnx:55\n==inference_images:43\n==inference_video:47\n==inference_webcam:38\nparser.add_argument('--model-type', type=str, required=True, choices=['mattingbase', 'mattingrefine'])\nparser.add_argument('--model-backbone', type=str, required=True, choices=['resnet101', 'resnet50', 'mobilenetv2'])\nparser.add_argument('--model-backbone-scale', type=float, default=0.25)\nparser.add_argument('--model-checkpoint', type=str, required=True)\nparser.add_argument('--model-refine-mode', type=str, default='sampling', choices=['full', 'sampling', 'thresholding'])\nparser.add_argument('--model-refine-sample-pixels', type=int, default=80_000)","18":"Similar lines in 2 files\n==train_base:100\n==train_refine:119\n    dataset_valid = ZipDataset([\n        ZipDataset([\n            ImagesDataset(DATA_PATH[args.dataset_name]['valid']['pha'], mode='L'),\n            ImagesDataset(DATA_PATH[args.dataset_name]['valid']['fgr'], mode='RGB')\n        ], transforms=A.PairCompose([","19":"Similar lines in 2 files\n==train_base:72\n==train_refine:87\n    dataset_train = ZipDataset([\n        ZipDataset([\n            ImagesDataset(DATA_PATH[args.dataset_name]['train']['pha'], mode='L'),\n            ImagesDataset(DATA_PATH[args.dataset_name]['train']['fgr'], mode='RGB'),\n        ], transforms=A.PairCompose(["},"number":{"0":"R0801","1":"R0801","2":"R0801","3":"R0801","4":"R0801","5":"R0801","6":"R0801","7":"R0801","8":"R0801","9":"R0801","10":"R0801","11":"R0801","12":"R0801","13":"R0801","14":"R0801","15":"R0801","16":"R0801","17":"R0801","18":"R0801","19":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint","17":"pylint","18":"pylint","19":"pylint"},"lines_amount":{"0":310,"1":310,"2":310,"3":310,"4":310,"5":310,"6":310,"7":310,"8":310,"9":310,"10":310,"11":310,"12":310,"13":310,"14":310,"15":310,"16":310,"17":310,"18":310,"19":310},"commit":{"0":"39aabd96fddc8cb60824711b5d2bbc5290af277c","1":"39aabd96fddc8cb60824711b5d2bbc5290af277c","2":"39aabd96fddc8cb60824711b5d2bbc5290af277c","3":"39aabd96fddc8cb60824711b5d2bbc5290af277c","4":"39aabd96fddc8cb60824711b5d2bbc5290af277c","5":"39aabd96fddc8cb60824711b5d2bbc5290af277c","6":"39aabd96fddc8cb60824711b5d2bbc5290af277c","7":"39aabd96fddc8cb60824711b5d2bbc5290af277c","8":"39aabd96fddc8cb60824711b5d2bbc5290af277c","9":"39aabd96fddc8cb60824711b5d2bbc5290af277c","10":"39aabd96fddc8cb60824711b5d2bbc5290af277c","11":"39aabd96fddc8cb60824711b5d2bbc5290af277c","12":"39aabd96fddc8cb60824711b5d2bbc5290af277c","13":"39aabd96fddc8cb60824711b5d2bbc5290af277c","14":"39aabd96fddc8cb60824711b5d2bbc5290af277c","15":"39aabd96fddc8cb60824711b5d2bbc5290af277c","16":"39aabd96fddc8cb60824711b5d2bbc5290af277c","17":"39aabd96fddc8cb60824711b5d2bbc5290af277c","18":"39aabd96fddc8cb60824711b5d2bbc5290af277c","19":"39aabd96fddc8cb60824711b5d2bbc5290af277c"},"repo":{"0":"PeterL1n\/BackgroundMattingV2","1":"PeterL1n\/BackgroundMattingV2","2":"PeterL1n\/BackgroundMattingV2","3":"PeterL1n\/BackgroundMattingV2","4":"PeterL1n\/BackgroundMattingV2","5":"PeterL1n\/BackgroundMattingV2","6":"PeterL1n\/BackgroundMattingV2","7":"PeterL1n\/BackgroundMattingV2","8":"PeterL1n\/BackgroundMattingV2","9":"PeterL1n\/BackgroundMattingV2","10":"PeterL1n\/BackgroundMattingV2","11":"PeterL1n\/BackgroundMattingV2","12":"PeterL1n\/BackgroundMattingV2","13":"PeterL1n\/BackgroundMattingV2","14":"PeterL1n\/BackgroundMattingV2","15":"PeterL1n\/BackgroundMattingV2","16":"PeterL1n\/BackgroundMattingV2","17":"PeterL1n\/BackgroundMattingV2","18":"PeterL1n\/BackgroundMattingV2","19":"PeterL1n\/BackgroundMattingV2"}}