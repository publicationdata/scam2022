{"type":{"0":"W","1":"R","2":"R","3":"R","4":"R","5":"R","6":"C","7":"C","8":"R","9":"R","10":"R","11":"R","12":"R","13":"R","14":"R","15":"R","16":"R","17":"R","18":"R"},"module":{"0":"coco_annotation","1":"coco_annotation","2":"coco_annotation","3":"convert","4":"kmeans","5":"kmeans","6":"train","7":"train_bottleneck","8":"voc_annotation","9":"voc_annotation","10":"voc_annotation","11":"yolo_video","12":"yolo_video","13":"yolo_video","14":"yolo_video","15":"yolo_video","16":"yolo_video","17":"yolo_video","18":"yolo_video"},"obj":{"0":"","1":"","2":"","3":"_main","4":"YOLO_Kmeans.result2txt","5":"YOLO_Kmeans.txt2boxes","6":"_main","7":"_main","8":"convert_annotation","9":"","10":"","11":"","12":"","13":"","14":"","15":"","16":"","17":"","18":""},"lnum":{"0":13,"1":6,"2":38,"3":73,"4":61,"5":72,"6":71,"7":92,"8":10,"9":27,"10":28,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1},"col":{"0":4,"1":4,"2":4,"3":19,"4":12,"5":12,"6":8,"7":8,"8":14,"9":16,"10":16,"11":0,"12":0,"13":0,"14":0,"15":0,"16":0,"17":0,"18":0},"filename":{"0":"coco_annotation.py","1":"coco_annotation.py","2":"coco_annotation.py","3":"convert.py","4":"kmeans.py","5":"kmeans.py","6":"train.py","7":"train_bottleneck.py","8":"voc_annotation.py","9":"voc_annotation.py","10":"voc_annotation.py","11":"yolo_video.py","12":"yolo_video.py","13":"yolo_video.py","14":"yolo_video.py","15":"yolo_video.py","16":"yolo_video.py","17":"yolo_video.py","18":"yolo_video.py"},"symbol":{"0":"redefined-builtin","1":"consider-using-with","2":"consider-using-with","3":"consider-using-with","4":"consider-using-with","5":"consider-using-with","6":"consider-using-enumerate","7":"consider-using-enumerate","8":"consider-using-with","9":"consider-using-with","10":"consider-using-with","11":"duplicate-code","12":"duplicate-code","13":"duplicate-code","14":"duplicate-code","15":"duplicate-code","16":"duplicate-code","17":"duplicate-code","18":"duplicate-code"},"text":{"0":"Redefining built-in 'id'","1":"Consider using 'with' for resource-allocating operations","2":"Consider using 'with' for resource-allocating operations","3":"Consider using 'with' for resource-allocating operations","4":"Consider using 'with' for resource-allocating operations","5":"Consider using 'with' for resource-allocating operations","6":"Consider using enumerate instead of iterating with range and len","7":"Consider using enumerate instead of iterating with range and len","8":"Consider using 'with' for resource-allocating operations","9":"Consider using 'with' for resource-allocating operations","10":"Consider using 'with' for resource-allocating operations","11":"Similar lines in 2 files\n==train:76\n==train_bottleneck:97\n        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n            steps_per_epoch=max(1, num_train\/\/batch_size),\n            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n            validation_steps=max(1, num_val\/\/batch_size),\n            epochs=100,\n            initial_epoch=50,\n            callbacks=[logging, checkpoint, reduce_lr, early_stopping])\n        model.save_weights(log_dir + 'trained_weights_final.h5')\n\n    # Further training if needed.\n\n\ndef get_classes(classes_path):\n    '''loads the classes'''\n    with open(classes_path) as f:\n        class_names = f.readlines()\n    class_names = [c.strip() for c in class_names]\n    return class_names\n\ndef get_anchors(anchors_path):\n    '''loads the anchors from a file'''\n    with open(anchors_path) as f:\n        anchors = f.readline()\n    anchors = [float(x) for x in anchors.split(',')]\n    return np.array(anchors).reshape(-1, 2)\n\n\ndef create_model(input_shape, anchors, num_classes, load_pretrained=True, freeze_body=2,\n            weights_path='model_data\/yolo_weights.h5'):\n    '''create the training model'''\n    K.clear_session() # get a new session\n    image_input = Input(shape=(None, None, 3))\n    h, w = input_shape\n    num_anchors = len(anchors)\n\n    y_true = [Input(shape=(h\/\/{0:32, 1:16, 2:8}[l], w\/\/{0:32, 1:16, 2:8}[l], \\\n        num_anchors\/\/3, num_classes+5)) for l in range(3)]\n\n    model_body = yolo_body(image_input, num_anchors\/\/3, num_classes)\n    print('Create YOLOv3 model with {} anchors and {} classes.'.format(num_anchors, num_classes))\n\n    if load_pretrained:\n        model_body.load_weights(weights_path, by_name=True, skip_mismatch=True)\n        print('Load weights {}.'.format(weights_path))\n        if freeze_body in [1, 2]:\n            # Freeze darknet53 body or freeze all but 3 output layers.\n            num = (185, len(model_body.layers)-3)[freeze_body-1]\n            for i in range(num): model_body.layers[i].trainable = False\n            print('Freeze the first {} layers of total {} layers.'.format(num, len(model_body.layers)))\n","12":"Similar lines in 2 files\n==train:32\n==train_bottleneck:27\n            freeze_body=2, weights_path='model_data\/yolo_weights.h5') # make sure you know what you freeze\n\n    logging = TensorBoard(log_dir=log_dir)\n    checkpoint = ModelCheckpoint(log_dir + 'ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5',\n        monitor='val_loss', save_weights_only=True, save_best_only=True, period=3)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1)\n    early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1)\n\n    val_split = 0.1\n    with open(annotation_path) as f:\n        lines = f.readlines()\n    np.random.seed(10101)\n    np.random.shuffle(lines)\n    np.random.seed(None)\n    num_val = int(len(lines)*val_split)\n    num_train = len(lines) - num_val\n\n    # Train with frozen layers first, to get a stable loss.\n    # Adjust num epochs to your dataset. This step is enough to obtain a not bad model.\n    if True:","13":"Similar lines in 2 files\n==train:57\n==train_bottleneck:78\n        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n                steps_per_epoch=max(1, num_train\/\/batch_size),\n                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n                validation_steps=max(1, num_val\/\/batch_size),\n                epochs=50,\n                initial_epoch=0,\n                callbacks=[logging, checkpoint])\n        model.save_weights(log_dir + 'trained_weights_stage_1.h5')\n\n    # Unfreeze and continue training, to fine-tune.\n    # Train longer if the result is not good.\n    if True:\n        for i in range(len(model.layers)):\n            model.layers[i].trainable = True\n        model.compile(optimizer=Adam(lr=1e-4), loss={'yolo_loss': lambda y_true, y_pred: y_pred}) # recompile to apply the change\n        print('Unfreeze all of the layers.')\n","14":"Similar lines in 2 files\n==train:4\n==train_bottleneck:4\nimport numpy as np\nimport keras.backend as K\nfrom keras.layers import Input, Lambda\nfrom keras.models import Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\nfrom yolo3.model import preprocess_true_boxes, yolo_body, tiny_yolo_body, yolo_loss\nfrom yolo3.utils import get_random_data\n\n\ndef _main():\n    annotation_path = 'train.txt'\n    log_dir = 'logs\/000\/'","15":"Similar lines in 2 files\n==train:19\n==train_bottleneck:19\n    anchors_path = 'model_data\/yolo_anchors.txt'\n    class_names = get_classes(classes_path)\n    num_classes = len(class_names)\n    anchors = get_anchors(anchors_path)\n\n    input_shape = (416,416) # multiple of 32, hw\n","16":"Similar lines in 2 files\n==train:166\n==train_bottleneck:177\n    n = len(annotation_lines)\n    i = 0\n    while True:\n        image_data = []\n        box_data = []\n        for b in range(batch_size):","17":"Similar lines in 2 files\n==train:76\n==train_bottleneck:78\n        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n                steps_per_epoch=max(1, num_train\/\/batch_size),\n                validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n                validation_steps=max(1, num_val\/\/batch_size),","18":"Similar lines in 2 files\n==train:57\n==train_bottleneck:97\n        print('Train on {} samples, val on {} samples, with batch size {}.'.format(num_train, num_val, batch_size))\n        model.fit_generator(data_generator_wrapper(lines[:num_train], batch_size, input_shape, anchors, num_classes),\n            steps_per_epoch=max(1, num_train\/\/batch_size),\n            validation_data=data_generator_wrapper(lines[num_train:], batch_size, input_shape, anchors, num_classes),\n            validation_steps=max(1, num_val\/\/batch_size),"},"number":{"0":"W0622","1":"R1732","2":"R1732","3":"R1732","4":"R1732","5":"R1732","6":"C0200","7":"C0200","8":"R1732","9":"R1732","10":"R1732","11":"R0801","12":"R0801","13":"R0801","14":"R0801","15":"R0801","16":"R0801","17":"R0801","18":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint","17":"pylint","18":"pylint"},"lines_amount":{"0":53,"1":53,"2":53,"3":263,"4":102,"5":102,"6":191,"7":223,"8":35,"9":35,"10":35,"11":78,"12":78,"13":78,"14":78,"15":78,"16":78,"17":78,"18":78},"commit":{"0":"e6598d13c703029b2686bc2eb8d5c09badf42992","1":"e6598d13c703029b2686bc2eb8d5c09badf42992","2":"e6598d13c703029b2686bc2eb8d5c09badf42992","3":"e6598d13c703029b2686bc2eb8d5c09badf42992","4":"e6598d13c703029b2686bc2eb8d5c09badf42992","5":"e6598d13c703029b2686bc2eb8d5c09badf42992","6":"e6598d13c703029b2686bc2eb8d5c09badf42992","7":"e6598d13c703029b2686bc2eb8d5c09badf42992","8":"e6598d13c703029b2686bc2eb8d5c09badf42992","9":"e6598d13c703029b2686bc2eb8d5c09badf42992","10":"e6598d13c703029b2686bc2eb8d5c09badf42992","11":"e6598d13c703029b2686bc2eb8d5c09badf42992","12":"e6598d13c703029b2686bc2eb8d5c09badf42992","13":"e6598d13c703029b2686bc2eb8d5c09badf42992","14":"e6598d13c703029b2686bc2eb8d5c09badf42992","15":"e6598d13c703029b2686bc2eb8d5c09badf42992","16":"e6598d13c703029b2686bc2eb8d5c09badf42992","17":"e6598d13c703029b2686bc2eb8d5c09badf42992","18":"e6598d13c703029b2686bc2eb8d5c09badf42992"},"repo":{"0":"qqwweee\/keras-yolo3","1":"qqwweee\/keras-yolo3","2":"qqwweee\/keras-yolo3","3":"qqwweee\/keras-yolo3","4":"qqwweee\/keras-yolo3","5":"qqwweee\/keras-yolo3","6":"qqwweee\/keras-yolo3","7":"qqwweee\/keras-yolo3","8":"qqwweee\/keras-yolo3","9":"qqwweee\/keras-yolo3","10":"qqwweee\/keras-yolo3","11":"qqwweee\/keras-yolo3","12":"qqwweee\/keras-yolo3","13":"qqwweee\/keras-yolo3","14":"qqwweee\/keras-yolo3","15":"qqwweee\/keras-yolo3","16":"qqwweee\/keras-yolo3","17":"qqwweee\/keras-yolo3","18":"qqwweee\/keras-yolo3"}}