{"type":{"0":"R","1":"R","2":"R","3":"R","4":"R","5":"R","6":"R"},"module":{"0":"run_rnn","1":"run_rnn","2":"run_rnn","3":"run_rnn","4":"run_rnn","5":"run_rnn","6":"run_rnn"},"obj":{"0":"","1":"","2":"","3":"","4":"","5":"","6":""},"lnum":{"0":1,"1":1,"2":1,"3":1,"4":1,"5":1,"6":1},"col":{"0":0,"1":0,"2":0,"3":0,"4":0,"5":0,"6":0},"filename":{"0":"run_rnn.py","1":"run_rnn.py","2":"run_rnn.py","3":"run_rnn.py","4":"run_rnn.py","5":"run_rnn.py","6":"run_rnn.py"},"symbol":{"0":"duplicate-code","1":"duplicate-code","2":"duplicate-code","3":"duplicate-code","4":"duplicate-code","5":"duplicate-code","6":"duplicate-code"},"text":{"0":"Similar lines in 2 files\n==run_cnn:63\n==run_rnn:62\n    if not os.path.exists(tensorboard_dir):\n        os.makedirs(tensorboard_dir)\n\n    tf.summary.scalar(\"loss\", model.loss)\n    tf.summary.scalar(\"accuracy\", model.acc)\n    merged_summary = tf.summary.merge_all()\n    writer = tf.summary.FileWriter(tensorboard_dir)\n\n    # \u914d\u7f6e Saver\n    saver = tf.train.Saver()\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    print(\"Loading training and validation data...\")\n    # \u8f7d\u5165\u8bad\u7ec3\u96c6\u4e0e\u9a8c\u8bc1\u96c6\n    start_time = time.time()\n    x_train, y_train = process_file(train_dir, word_to_id, cat_to_id, config.seq_length)\n    x_val, y_val = process_file(val_dir, word_to_id, cat_to_id, config.seq_length)\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n\n    # \u521b\u5efasession\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    writer.add_graph(session.graph)\n\n    print('Training and evaluating...')\n    start_time = time.time()\n    total_batch = 0  # \u603b\u6279\u6b21\n    best_acc_val = 0.0  # \u6700\u4f73\u9a8c\u8bc1\u96c6\u51c6\u786e\u7387\n    last_improved = 0  # \u8bb0\u5f55\u4e0a\u4e00\u6b21\u63d0\u5347\u6279\u6b21\n    require_improvement = 1000  # \u5982\u679c\u8d85\u8fc71000\u8f6e\u672a\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n\n    flag = False\n    for epoch in range(config.num_epochs):\n        print('Epoch:', epoch + 1)\n        batch_train = batch_iter(x_train, y_train, config.batch_size)\n        for x_batch, y_batch in batch_train:\n            feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n\n            if total_batch % config.save_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u5c06\u8bad\u7ec3\u7ed3\u679c\u5199\u5165tensorboard scalar\n                s = session.run(merged_summary, feed_dict=feed_dict)\n                writer.add_summary(s, total_batch)\n\n            if total_batch % config.print_per_batch == 0:\n                # \u6bcf\u591a\u5c11\u8f6e\u6b21\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6027\u80fd\n                feed_dict[model.keep_prob] = 1.0\n                loss_train, acc_train = session.run([model.loss, model.acc], feed_dict=feed_dict)\n                loss_val, acc_val = evaluate(session, x_val, y_val)  # todo\n\n                if acc_val > best_acc_val:\n                    # \u4fdd\u5b58\u6700\u597d\u7ed3\u679c\n                    best_acc_val = acc_val\n                    last_improved = total_batch\n                    saver.save(sess=session, save_path=save_path)\n                    improved_str = '*'\n                else:\n                    improved_str = ''\n\n                time_dif = get_time_dif(start_time)\n                msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n                print(msg.format(total_batch, loss_train, acc_train, loss_val, acc_val, time_dif, improved_str))\n\n            feed_dict[model.keep_prob] = config.dropout_keep_prob\n            session.run(model.optim, feed_dict=feed_dict)  # \u8fd0\u884c\u4f18\u5316\n            total_batch += 1\n\n            if total_batch - last_improved > require_improvement:\n                # \u9a8c\u8bc1\u96c6\u6b63\u786e\u7387\u957f\u671f\u4e0d\u63d0\u5347\uff0c\u63d0\u524d\u7ed3\u675f\u8bad\u7ec3\n                print(\"No optimization for a long time, auto-stopping...\")\n                flag = True\n                break  # \u8df3\u51fa\u5faa\u73af\n        if flag:  # \u540c\u4e0a\n            break\n\n\ndef test():\n    print(\"Loading test data...\")\n    start_time = time.time()\n    x_test, y_test = process_file(test_dir, word_to_id, cat_to_id, config.seq_length)\n\n    session = tf.Session()\n    session.run(tf.global_variables_initializer())\n    saver = tf.train.Saver()\n    saver.restore(sess=session, save_path=save_path)  # \u8bfb\u53d6\u4fdd\u5b58\u7684\u6a21\u578b\n\n    print('Testing...')\n    loss_test, acc_test = evaluate(session, x_test, y_test)\n    msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n    print(msg.format(loss_test, acc_test))\n\n    batch_size = 128\n    data_len = len(x_test)\n    num_batch = int((data_len - 1) \/ batch_size) + 1\n\n    y_test_cls = np.argmax(y_test, 1)\n    y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)  # \u4fdd\u5b58\u9884\u6d4b\u7ed3\u679c\n    for i in range(num_batch):  # \u9010\u6279\u6b21\u5904\u7406\n        start_id = i * batch_size\n        end_id = min((i + 1) * batch_size, data_len)\n        feed_dict = {\n            model.input_x: x_test[start_id:end_id],\n            model.keep_prob: 1.0\n        }\n        y_pred_cls[start_id:end_id] = session.run(model.y_pred_cls, feed_dict=feed_dict)\n\n    # \u8bc4\u4f30\n    print(\"Precision, Recall and F1-Score...\")\n    print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n\n    # \u6df7\u6dc6\u77e9\u9635\n    print(\"Confusion Matrix...\")\n    cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n    print(cm)\n\n    time_dif = get_time_dif(start_time)\n    print(\"Time usage:\", time_dif)\n\n\nif __name__ == '__main__':\n    if len(sys.argv) != 2 or sys.argv[1] not in ['train', 'test']:","1":"Similar lines in 2 files\n==run_cnn:24\n==run_rnn:23\nsave_path = os.path.join(save_dir, 'best_validation')  # \u6700\u4f73\u9a8c\u8bc1\u7ed3\u679c\u4fdd\u5b58\u8def\u5f84\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n\n\ndef feed_data(x_batch, y_batch, keep_prob):\n    feed_dict = {\n        model.input_x: x_batch,\n        model.input_y: y_batch,\n        model.keep_prob: keep_prob\n    }\n    return feed_dict\n\n\ndef evaluate(sess, x_, y_):\n    \"\"\"\u8bc4\u4f30\u5728\u67d0\u4e00\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\u548c\u635f\u5931\"\"\"\n    data_len = len(x_)\n    batch_eval = batch_iter(x_, y_, 128)\n    total_loss = 0.0\n    total_acc = 0.0\n    for x_batch, y_batch in batch_eval:\n        batch_len = len(x_batch)\n        feed_dict = feed_data(x_batch, y_batch, 1.0)\n        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n        total_loss += loss * batch_len\n        total_acc += acc * batch_len\n\n    return total_loss \/ data_len, total_acc \/ data_len\n\n\ndef train():\n    print(\"Configuring TensorBoard and Saver...\")\n    # \u914d\u7f6e Tensorboard\uff0c\u91cd\u65b0\u8bad\u7ec3\u65f6\uff0c\u8bf7\u5c06tensorboard\u6587\u4ef6\u5939\u5220\u9664\uff0c\u4e0d\u7136\u56fe\u4f1a\u8986\u76d6","2":"Similar lines in 2 files\n==cnn_model:56\n==rnn_model:72\n            fc = tf.contrib.layers.dropout(fc, self.keep_prob)\n            fc = tf.nn.relu(fc)\n\n            # \u5206\u7c7b\u5668\n            self.logits = tf.layers.dense(fc, self.config.num_classes, name='fc2')\n            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)  # \u9884\u6d4b\u7c7b\u522b\n\n        with tf.name_scope(\"optimize\"):\n            # \u635f\u5931\u51fd\u6570\uff0c\u4ea4\u53c9\u71b5\n            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits, labels=self.input_y)\n            self.loss = tf.reduce_mean(cross_entropy)\n            # \u4f18\u5316\u5668\n            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n\n        with tf.name_scope(\"accuracy\"):\n            # \u51c6\u786e\u7387\n            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))","3":"Similar lines in 2 files\n==run_cnn:3\n==run_rnn:2\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport time\nfrom datetime import timedelta\n\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn import metrics\n","4":"Similar lines in 2 files\n==run_cnn:15\n==run_rnn:14\nfrom data.cnews_loader import read_vocab, read_category, batch_iter, process_file, build_vocab\n\nbase_dir = 'data\/cnews'\ntrain_dir = os.path.join(base_dir, 'cnews.train.txt')\ntest_dir = os.path.join(base_dir, 'cnews.test.txt')\nval_dir = os.path.join(base_dir, 'cnews.val.txt')\nvocab_dir = os.path.join(base_dir, 'cnews.vocab.txt')\n","5":"Similar lines in 2 files\n==cnn_model:30\n==rnn_model:30\n    def __init__(self, config):\n        self.config = config\n\n        # \u4e09\u4e2a\u5f85\u8f93\u5165\u7684\u6570\u636e\n        self.input_x = tf.placeholder(tf.int32, [None, self.config.seq_length], name='input_x')\n        self.input_y = tf.placeholder(tf.float32, [None, self.config.num_classes], name='input_y')\n        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n","6":"Similar lines in 2 files\n==run_cnn:190\n==run_rnn:189\n    if not os.path.exists(vocab_dir):  # \u5982\u679c\u4e0d\u5b58\u5728\u8bcd\u6c47\u8868\uff0c\u91cd\u5efa\n        build_vocab(train_dir, vocab_dir, config.vocab_size)\n    categories, cat_to_id = read_category()\n    words, word_to_id = read_vocab(vocab_dir)\n    config.vocab_size = len(words)"},"number":{"0":"R0801","1":"R0801","2":"R0801","3":"R0801","4":"R0801","5":"R0801","6":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint"},"lines_amount":{"0":201,"1":201,"2":201,"3":201,"4":201,"5":201,"6":201},"commit":{"0":"90dd55cccacac663e87445ed356bf59307864168","1":"90dd55cccacac663e87445ed356bf59307864168","2":"90dd55cccacac663e87445ed356bf59307864168","3":"90dd55cccacac663e87445ed356bf59307864168","4":"90dd55cccacac663e87445ed356bf59307864168","5":"90dd55cccacac663e87445ed356bf59307864168","6":"90dd55cccacac663e87445ed356bf59307864168"},"repo":{"0":"gaussic\/text-classification-cnn-rnn","1":"gaussic\/text-classification-cnn-rnn","2":"gaussic\/text-classification-cnn-rnn","3":"gaussic\/text-classification-cnn-rnn","4":"gaussic\/text-classification-cnn-rnn","5":"gaussic\/text-classification-cnn-rnn","6":"gaussic\/text-classification-cnn-rnn"}}