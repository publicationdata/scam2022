{"type":{"0":"R","1":"C","2":"C","3":"W","4":"C","5":"W","6":"C","7":"C","8":"W","9":"W","10":"C","11":"C","12":"C","13":"R","14":"C","15":"W","16":"W","17":"R","18":"R","19":"R","20":"R","21":"R"},"module":{"0":"data_loader","1":"functions","2":"loss_functions","3":"networks","4":"networks","5":"networks","6":"networks","7":"networks","8":"networks","9":"test_background-matting_image","10":"test_background-matting_image","11":"test_pre_process","12":"test_pre_process_video","13":"test_segmentation_deeplab","14":"test_segmentation_deeplab","15":"train_adobe","16":"train_real_fixed","17":"train_real_fixed","18":"train_real_fixed","19":"train_real_fixed","20":"train_real_fixed","21":"train_real_fixed"},"obj":{"0":"AdobeDataAffineHR.__getitem__","1":"crop_images","2":"GANloss.forward","3":"MultiscaleDiscriminator.singleD_forward","4":"MultiscaleDiscriminator.singleD_forward","5":"MultiscaleDiscriminator.forward","6":"NLayerDiscriminator.__init__","7":"NLayerDiscriminator.__init__","8":"NLayerDiscriminator.forward","9":"","10":"","11":"","12":"","13":"DeepLabModel.__init__","14":"","15":"","16":"","17":"","18":"","19":"","20":"","21":""},"lnum":{"0":79,"1":50,"2":74,"3":267,"4":270,"5":276,"6":324,"7":328,"8":332,"9":15,"10":78,"11":100,"12":118,"13":28,"14":162,"15":15,"16":16,"17":1,"18":1,"19":1,"20":1,"21":1},"col":{"0":1,"1":4,"2":2,"3":34,"4":3,"5":19,"6":3,"7":3,"8":19,"9":0,"10":0,"11":0,"12":0,"13":13,"14":0,"15":0,"16":0,"17":0,"18":0,"19":0,"20":0,"21":0},"filename":{"0":"data_loader.py","1":"functions.py","2":"loss_functions.py","3":"networks.py","4":"networks.py","5":"networks.py","6":"networks.py","7":"networks.py","8":"networks.py","9":"test_background-matting_image.py","10":"test_background-matting_image.py","11":"test_pre_process.py","12":"test_pre_process_video.py","13":"test_segmentation_deeplab.py","14":"test_segmentation_deeplab.py","15":"train_adobe.py","16":"train_real_fixed.py","17":"train_real_fixed.py","18":"train_real_fixed.py","19":"train_real_fixed.py","20":"train_real_fixed.py","21":"train_real_fixed.py"},"symbol":{"0":"inconsistent-return-statements","1":"consider-using-enumerate","2":"consider-using-enumerate","3":"redefined-builtin","4":"consider-using-enumerate","5":"redefined-builtin","6":"consider-using-enumerate","7":"consider-using-enumerate","8":"redefined-builtin","9":"wildcard-import","10":"consider-using-enumerate","11":"consider-using-enumerate","12":"consider-using-enumerate","13":"consider-using-with","14":"consider-using-enumerate","15":"wildcard-import","16":"wildcard-import","17":"duplicate-code","18":"duplicate-code","19":"duplicate-code","20":"duplicate-code","21":"duplicate-code"},"text":{"0":"Either all return statements in a function should return an expression, or none of them should.","1":"Consider using enumerate instead of iterating with range and len","2":"Consider using enumerate instead of iterating with range and len","3":"Redefining built-in 'input'","4":"Consider using enumerate instead of iterating with range and len","5":"Redefining built-in 'input'","6":"Consider using enumerate instead of iterating with range and len","7":"Consider using enumerate instead of iterating with range and len","8":"Redefining built-in 'input'","9":"Wildcard import functions","10":"Consider using enumerate instead of iterating with range and len","11":"Consider using enumerate instead of iterating with range and len","12":"Consider using enumerate instead of iterating with range and len","13":"Consider using 'with' for resource-allocating operations","14":"Consider using enumerate instead of iterating with range and len","15":"Wildcard import functions","16":"Wildcard import functions","17":"Similar lines in 2 files\n==test_pre_process:0\n==test_pre_process_video:0\nimport numpy as np\nimport cv2, pdb, glob, argparse\n\nMAX_FEATURES = 500\nGOOD_MATCH_PERCENT = 0.15\n\n\ndef alignImages(im1, im2,masksDL):\n\n\t# Convert images to grayscale\n\tim1Gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n\tim2Gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)\n\n\takaze = cv2.AKAZE_create()\n\tkeypoints1, descriptors1 = akaze.detectAndCompute(im1, None)\n\tkeypoints2, descriptors2 = akaze.detectAndCompute(im2, None)\n\n\t# Match features.\n\tmatcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE)\n\tmatches = matcher.match(descriptors1, descriptors2, None)\n\n\t# Sort matches by score\n\tmatches.sort(key=lambda x: x.distance, reverse=False)\n\n\t# Remove not so good matches\n\tnumGoodMatches = int(len(matches) * GOOD_MATCH_PERCENT)\n\tmatches = matches[:numGoodMatches]\n\n\t# Extract location of good matches\n\tpoints1 = np.zeros((len(matches), 2), dtype=np.float32)\n\tpoints2 = np.zeros((len(matches), 2), dtype=np.float32)\n\n\tfor i, match in enumerate(matches):\n\t\tpoints1[i, :] = keypoints1[match.queryIdx].pt\n\t\tpoints2[i, :] = keypoints2[match.trainIdx].pt\n\n\t# Find homography\n\th, mask = cv2.findHomography(points1, points2, cv2.RANSAC)\n\n\t# Use homography\n\theight, width, channels = im2.shape\n\tim1Reg = cv2.warpPerspective(im1, h, (width, height))\n\t# copy image in the empty region, unless it is a foreground. Then copy background\n\n\tmask_rep=(np.sum(im1Reg.astype('float32'),axis=2)==0)\n\n\tim1Reg[mask_rep,0]=im2[mask_rep,0]\n\tim1Reg[mask_rep,1]=im2[mask_rep,1]\n\tim1Reg[mask_rep,2]=im2[mask_rep,2]\n\n\tmask_rep1=np.logical_and(mask_rep , masksDL[...,0]==255)\n\n\tim1Reg[mask_rep1,0]=im1[mask_rep1,0]\n\tim1Reg[mask_rep1,1]=im1[mask_rep1,1]\n\tim1Reg[mask_rep1,2]=im1[mask_rep1,2]\n\n\n\treturn im1Reg\n","18":"Similar lines in 2 files\n==train_adobe:0\n==train_real_fixed:0\nfrom __future__ import print_function\n\nimport torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tensorboardX import SummaryWriter\n\n\nimport os\nimport time\nimport argparse","19":"Similar lines in 2 files\n==data_loader:160\n==test_background-matting_image:127\n\tkernel_er = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n\tkernel_dil = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5, 5))\n\trcnn=rcnn.astype(np.float32)\/255; rcnn[rcnn>0.2]=1;\n\tK=25\n\n\tzero_id=np.nonzero(np.sum(rcnn,axis=1)==0)\n\tdel_id=zero_id[0][zero_id[0]>250]\n\tif len(del_id)>0:\n\t\tdel_id=[del_id[0]-2,del_id[0]-1,*del_id]\n\t\trcnn=np.delete(rcnn,del_id,0)\n\trcnn = cv2.copyMakeBorder( rcnn, 0, K + len(del_id), 0, 0, cv2.BORDER_REPLICATE)\n","20":"Similar lines in 2 files\n==train_adobe:22\n==train_real_fixed:23\nprint('CUDA Device: ' + os.environ[\"CUDA_VISIBLE_DEVICES\"])\n\n\n\"\"\"Parses arguments.\"\"\"\nparser = argparse.ArgumentParser(description='Training Background Matting on Adobe Dataset.')\nparser.add_argument('-n', '--name', type=str, help='Name of tensorboard and model saving folders.')\nparser.add_argument('-bs', '--batch_size', type=int, help='Batch Size.')\nparser.add_argument('-res', '--reso', type=int, help='Input image resolution')","21":"Similar lines in 2 files\n==test_pre_process:60\n==test_pre_process_video:59\ndef adjustExposure(img,back,mask):\n\tkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n\tmask = cv2.dilate(mask, kernel, iterations=10)\n\tmask1 = cv2.dilate(mask, kernel, iterations=300)\n\tmsk=mask1.astype(np.float32)\/255-mask.astype(np.float32)\/255; msk=msk.astype(np.bool)\n"},"number":{"0":"R1710","1":"C0200","2":"C0200","3":"W0622","4":"C0200","5":"W0622","6":"C0200","7":"C0200","8":"W0622","9":"W0401","10":"C0200","11":"C0200","12":"C0200","13":"R1732","14":"C0200","15":"W0401","16":"W0401","17":"R0801","18":"R0801","19":"R0801","20":"R0801","21":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint","17":"pylint","18":"pylint","19":"pylint","20":"pylint","21":"pylint"},"lines_amount":{"0":317,"1":111,"2":94,"3":343,"4":343,"5":343,"6":343,"7":343,"8":343,"9":199,"10":199,"11":113,"12":131,"13":180,"14":180,"15":183,"16":244,"17":244,"18":244,"19":244,"20":244,"21":244},"commit":{"0":"90765850422621f38ec9f912059cfcad1a8a592b","1":"90765850422621f38ec9f912059cfcad1a8a592b","2":"90765850422621f38ec9f912059cfcad1a8a592b","3":"90765850422621f38ec9f912059cfcad1a8a592b","4":"90765850422621f38ec9f912059cfcad1a8a592b","5":"90765850422621f38ec9f912059cfcad1a8a592b","6":"90765850422621f38ec9f912059cfcad1a8a592b","7":"90765850422621f38ec9f912059cfcad1a8a592b","8":"90765850422621f38ec9f912059cfcad1a8a592b","9":"90765850422621f38ec9f912059cfcad1a8a592b","10":"90765850422621f38ec9f912059cfcad1a8a592b","11":"90765850422621f38ec9f912059cfcad1a8a592b","12":"90765850422621f38ec9f912059cfcad1a8a592b","13":"90765850422621f38ec9f912059cfcad1a8a592b","14":"90765850422621f38ec9f912059cfcad1a8a592b","15":"90765850422621f38ec9f912059cfcad1a8a592b","16":"90765850422621f38ec9f912059cfcad1a8a592b","17":"90765850422621f38ec9f912059cfcad1a8a592b","18":"90765850422621f38ec9f912059cfcad1a8a592b","19":"90765850422621f38ec9f912059cfcad1a8a592b","20":"90765850422621f38ec9f912059cfcad1a8a592b","21":"90765850422621f38ec9f912059cfcad1a8a592b"},"repo":{"0":"senguptaumd\/Background-Matting","1":"senguptaumd\/Background-Matting","2":"senguptaumd\/Background-Matting","3":"senguptaumd\/Background-Matting","4":"senguptaumd\/Background-Matting","5":"senguptaumd\/Background-Matting","6":"senguptaumd\/Background-Matting","7":"senguptaumd\/Background-Matting","8":"senguptaumd\/Background-Matting","9":"senguptaumd\/Background-Matting","10":"senguptaumd\/Background-Matting","11":"senguptaumd\/Background-Matting","12":"senguptaumd\/Background-Matting","13":"senguptaumd\/Background-Matting","14":"senguptaumd\/Background-Matting","15":"senguptaumd\/Background-Matting","16":"senguptaumd\/Background-Matting","17":"senguptaumd\/Background-Matting","18":"senguptaumd\/Background-Matting","19":"senguptaumd\/Background-Matting","20":"senguptaumd\/Background-Matting","21":"senguptaumd\/Background-Matting"}}