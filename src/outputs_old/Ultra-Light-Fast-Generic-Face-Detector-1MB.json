{"type":{"0":"W","1":"R","2":"R","3":"R","4":"W","5":"R","6":"W","7":"R","8":"W","9":"R","10":"W","11":"R","12":"W","13":"R","14":"R","15":"R","16":"R"},"module":{"0":"check_gt_box","1":"check_gt_box","2":"check_gt_box","3":"convert_to_onnx","4":"detect_imgs","5":"detect_imgs","6":"detect_imgs_onnx","7":"detect_imgs_onnx","8":"run_video_face_detect","9":"run_video_face_detect","10":"run_video_face_detect_onnx","11":"run_video_face_detect_onnx","12":"train","13":"train","14":"train","15":"train","16":"train"},"obj":{"0":"","1":"","2":"","3":"","4":"","5":"","6":"","7":"","8":"","9":"","10":"","11":"","12":"lr_poly","13":"","14":"","15":"","16":""},"lnum":{"0":19,"1":20,"2":23,"3":19,"4":56,"5":37,"6":62,"7":49,"8":66,"9":43,"10":62,"11":48,"12":121,"13":1,"14":1,"15":1,"16":1},"col":{"0":19,"1":20,"2":25,"3":40,"4":0,"5":40,"6":0,"7":40,"8":0,"9":40,"10":0,"11":40,"12":21,"13":0,"14":0,"15":0,"16":0},"filename":{"0":"check_gt_box.py","1":"check_gt_box.py","2":"check_gt_box.py","3":"convert_to_onnx.py","4":"detect_imgs.py","5":"detect_imgs.py","6":"detect_imgs_onnx.py","7":"detect_imgs_onnx.py","8":"run_video_face_detect.py","9":"run_video_face_detect.py","10":"run_video_face_detect_onnx.py","11":"run_video_face_detect_onnx.py","12":"train.py","13":"train.py","14":"train.py","15":"train.py","16":"train.py"},"symbol":{"0":"redefined-builtin","1":"consider-using-with","2":"consider-using-with","3":"consider-using-with","4":"redefined-builtin","5":"consider-using-with","6":"redefined-builtin","7":"consider-using-with","8":"redefined-builtin","9":"consider-using-with","10":"redefined-builtin","11":"consider-using-with","12":"redefined-builtin","13":"duplicate-code","14":"duplicate-code","15":"duplicate-code","16":"duplicate-code"},"text":{"0":"Redefining built-in 'set'","1":"Consider using 'with' for resource-allocating operations","2":"Consider using 'with' for resource-allocating operations","3":"Consider using 'with' for resource-allocating operations","4":"Redefining built-in 'sum'","5":"Consider using 'with' for resource-allocating operations","6":"Redefining built-in 'sum'","7":"Consider using 'with' for resource-allocating operations","8":"Redefining built-in 'sum'","9":"Consider using 'with' for resource-allocating operations","10":"Redefining built-in 'sum'","11":"Consider using 'with' for resource-allocating operations","12":"Redefining built-in 'iter'","13":"Similar lines in 2 files\n==detect_imgs_onnx:4\n==run_video_face_detect_onnx:3\nimport time\n\nimport cv2\nimport numpy as np\nimport onnx\nimport vision.utils.box_utils_numpy as box_utils\nfrom caffe2.python.onnx import backend\n\n# onnx runtime\nimport onnxruntime as ort\n\n\ndef predict(width, height, confidences, boxes, prob_threshold, iou_threshold=0.3, top_k=-1):\n    boxes = boxes[0]\n    confidences = confidences[0]\n    picked_box_probs = []\n    picked_labels = []\n    for class_index in range(1, confidences.shape[1]):\n        probs = confidences[:, class_index]\n        mask = probs > prob_threshold\n        probs = probs[mask]\n        if probs.shape[0] == 0:\n            continue\n        subset_boxes = boxes[mask, :]\n        box_probs = np.concatenate([subset_boxes, probs.reshape(-1, 1)], axis=1)\n        box_probs = box_utils.hard_nms(box_probs,\n                                       iou_threshold=iou_threshold,\n                                       top_k=top_k,\n                                       )\n        picked_box_probs.append(box_probs)\n        picked_labels.extend([class_index] * box_probs.shape[0])\n    if not picked_box_probs:\n        return np.array([]), np.array([]), np.array([])\n    picked_box_probs = np.concatenate(picked_box_probs)\n    picked_box_probs[:, 0] *= width\n    picked_box_probs[:, 1] *= height\n    picked_box_probs[:, 2] *= width\n    picked_box_probs[:, 3] *= height\n    return picked_box_probs[:, :4].astype(np.int32), np.array(picked_labels), picked_box_probs[:, 4]\n\n\nlabel_path = \"models\/voc-model-labels.txt\"\n\nonnx_path = \"models\/onnx\/version-RFB-320.onnx\"\nclass_names = [name.strip() for name in open(label_path).readlines()]\n\npredictor = onnx.load(onnx_path)\nonnx.checker.check_model(predictor)\nonnx.helper.printable_graph(predictor.graph)\npredictor = backend.prepare(predictor, device=\"CPU\")  # default CPU\n\nort_session = ort.InferenceSession(onnx_path)\ninput_name = ort_session.get_inputs()[0].name","14":"Similar lines in 2 files\n==detect_imgs_onnx:69\n==run_video_face_detect_onnx:67\n    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB)\n    image = cv2.resize(image, (320, 240))\n    # image = cv2.resize(image, (640, 480))\n    image_mean = np.array([127, 127, 127])\n    image = (image - image_mean) \/ 128\n    image = np.transpose(image, [2, 0, 1])\n    image = np.expand_dims(image, axis=0)\n    image = image.astype(np.float32)\n    # confidences, boxes = predictor.run(image)\n    time_time = time.time()\n    confidences, boxes = ort_session.run(None, {input_name: image})\n    print(\"cost time:{}\".format(time.time() - time_time))\n    boxes, labels, probs = predict(orig_image.shape[1], orig_image.shape[0], confidences, boxes, threshold)\n    for i in range(boxes.shape[0]):\n        box = boxes[i, :]\n        label = f\"{class_names[labels[i]]}: {probs[i]:.2f}\"\n\n        cv2.rectangle(orig_image, (box[0], box[1]), (box[2], box[3]), (255, 255, 0), 4)\n\n        # cv2.putText(orig_image, label,\n        #             (box[0] + 20, box[1] + 40),\n        #             cv2.FONT_HERSHEY_SIMPLEX,\n        #             1,  # font scale\n        #             (255, 0, 255),\n        #             2)  # line type","15":"Similar lines in 2 files\n==run_video_face_detect:89\n==run_video_face_detect_onnx:94\n    cv2.imshow('annotated', orig_image)\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\ncap.release()\ncv2.destroyAllWindows()","16":"Similar lines in 2 files\n==detect_imgs:21\n==run_video_face_detect:19\n                    help='nms candidate size')\nparser.add_argument('--path', default=\"imgs\", type=str,\n                    help='imgs dir')\nparser.add_argument('--test_device', default=\"cuda:0\", type=str,\n                    help='cuda:0 or cpu')"},"number":{"0":"W0622","1":"R1732","2":"R1732","3":"R1732","4":"W0622","5":"R1732","6":"W0622","7":"R1732","8":"W0622","9":"R1732","10":"W0622","11":"R1732","12":"W0622","13":"R0801","14":"R0801","15":"R0801","16":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint"},"lines_amount":{"0":60,"1":60,"2":60,"3":44,"4":73,"5":73,"6":98,"7":98,"8":96,"9":96,"10":101,"11":101,"12":361,"13":361,"14":361,"15":361,"16":361},"commit":{"0":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","1":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","2":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","3":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","4":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","5":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","6":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","7":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","8":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","9":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","10":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","11":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","12":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","13":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","14":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","15":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3","16":"f0578bcb789a81f9e03d6c8aa999b71872f0d1a3"},"repo":{"0":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","1":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","2":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","3":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","4":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","5":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","6":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","7":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","8":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","9":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","10":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","11":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","12":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","13":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","14":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","15":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB","16":"Linzaer\/Ultra-Light-Fast-Generic-Face-Detector-1MB"}}