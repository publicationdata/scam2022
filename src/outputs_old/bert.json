{"type":{"0":"R","1":"R","2":"C","3":"R","4":"R","5":"R","6":"R","7":"R","8":"R","9":"R","10":"R","11":"R","12":"R","13":"R","14":"R","15":"R","16":"R","17":"R","18":"R","19":"R","20":"R","21":"R","22":"R","23":"R","24":"R","25":"R","26":"R","27":"R","28":"R","29":"R","30":"R","31":"R","32":"R","33":"R","34":"R","35":"R","36":"R","37":"R","38":"R","39":"R","40":"R","41":"R","42":"R","43":"R","44":"R","45":"R","46":"R","47":"R","48":"R","49":"R","50":"R","51":"R","52":"R"},"module":{"0":"create_pretraining_data","1":"modeling_test","2":"run_squad","3":"tokenization","4":"tokenization","5":"tokenization_test","6":"tokenization_test","7":"tokenization_test","8":"tokenization_test","9":"tokenization_test","10":"tokenization_test","11":"tokenization_test","12":"tokenization_test","13":"tokenization_test","14":"tokenization_test","15":"tokenization_test","16":"tokenization_test","17":"tokenization_test","18":"tokenization_test","19":"tokenization_test","20":"tokenization_test","21":"tokenization_test","22":"tokenization_test","23":"tokenization_test","24":"tokenization_test","25":"tokenization_test","26":"tokenization_test","27":"tokenization_test","28":"tokenization_test","29":"tokenization_test","30":"tokenization_test","31":"tokenization_test","32":"tokenization_test","33":"tokenization_test","34":"tokenization_test","35":"tokenization_test","36":"tokenization_test","37":"tokenization_test","38":"tokenization_test","39":"tokenization_test","40":"tokenization_test","41":"tokenization_test","42":"tokenization_test","43":"tokenization_test","44":"tokenization_test","45":"tokenization_test","46":"tokenization_test","47":"tokenization_test","48":"tokenization_test","49":"tokenization_test","50":"tokenization_test","51":"tokenization_test","52":"tokenization_test"},"obj":{"0":"create_masked_lm_predictions","1":"BertModelTest.get_unreachable_ops","2":"_get_best_indexes","3":"_is_whitespace","4":"_is_control","5":"","6":"","7":"","8":"","9":"","10":"","11":"","12":"","13":"","14":"","15":"","16":"","17":"","18":"","19":"","20":"","21":"","22":"","23":"","24":"","25":"","26":"","27":"","28":"","29":"","30":"","31":"","32":"","33":"","34":"","35":"","36":"","37":"","38":"","39":"","40":"","41":"","42":"","43":"","44":"","45":"","46":"","47":"","48":"","49":"","50":"","51":"","52":""},"lnum":{"0":348,"1":194,"2":1028,"3":366,"4":378,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1,"17":1,"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"39":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1,"50":1,"51":1,"52":1},"col":{"0":7,"1":2,"2":2,"3":5,"4":5,"5":0,"6":0,"7":0,"8":0,"9":0,"10":0,"11":0,"12":0,"13":0,"14":0,"15":0,"16":0,"17":0,"18":0,"19":0,"20":0,"21":0,"22":0,"23":0,"24":0,"25":0,"26":0,"27":0,"28":0,"29":0,"30":0,"31":0,"32":0,"33":0,"34":0,"35":0,"36":0,"37":0,"38":0,"39":0,"40":0,"41":0,"42":0,"43":0,"44":0,"45":0,"46":0,"47":0,"48":0,"49":0,"50":0,"51":0,"52":0},"filename":{"0":"create_pretraining_data.py","1":"modeling_test.py","2":"run_squad.py","3":"tokenization.py","4":"tokenization.py","5":"tokenization_test.py","6":"tokenization_test.py","7":"tokenization_test.py","8":"tokenization_test.py","9":"tokenization_test.py","10":"tokenization_test.py","11":"tokenization_test.py","12":"tokenization_test.py","13":"tokenization_test.py","14":"tokenization_test.py","15":"tokenization_test.py","16":"tokenization_test.py","17":"tokenization_test.py","18":"tokenization_test.py","19":"tokenization_test.py","20":"tokenization_test.py","21":"tokenization_test.py","22":"tokenization_test.py","23":"tokenization_test.py","24":"tokenization_test.py","25":"tokenization_test.py","26":"tokenization_test.py","27":"tokenization_test.py","28":"tokenization_test.py","29":"tokenization_test.py","30":"tokenization_test.py","31":"tokenization_test.py","32":"tokenization_test.py","33":"tokenization_test.py","34":"tokenization_test.py","35":"tokenization_test.py","36":"tokenization_test.py","37":"tokenization_test.py","38":"tokenization_test.py","39":"tokenization_test.py","40":"tokenization_test.py","41":"tokenization_test.py","42":"tokenization_test.py","43":"tokenization_test.py","44":"tokenization_test.py","45":"tokenization_test.py","46":"tokenization_test.py","47":"tokenization_test.py","48":"tokenization_test.py","49":"tokenization_test.py","50":"tokenization_test.py","51":"tokenization_test.py","52":"tokenization_test.py"},"symbol":{"0":"consider-using-in","1":"too-many-branches","2":"consider-using-enumerate","3":"consider-using-in","4":"consider-using-in","5":"duplicate-code","6":"duplicate-code","7":"duplicate-code","8":"duplicate-code","9":"duplicate-code","10":"duplicate-code","11":"duplicate-code","12":"duplicate-code","13":"duplicate-code","14":"duplicate-code","15":"duplicate-code","16":"duplicate-code","17":"duplicate-code","18":"duplicate-code","19":"duplicate-code","20":"duplicate-code","21":"duplicate-code","22":"duplicate-code","23":"duplicate-code","24":"duplicate-code","25":"duplicate-code","26":"duplicate-code","27":"duplicate-code","28":"duplicate-code","29":"duplicate-code","30":"duplicate-code","31":"duplicate-code","32":"duplicate-code","33":"duplicate-code","34":"duplicate-code","35":"duplicate-code","36":"duplicate-code","37":"duplicate-code","38":"duplicate-code","39":"duplicate-code","40":"duplicate-code","41":"duplicate-code","42":"duplicate-code","43":"duplicate-code","44":"duplicate-code","45":"duplicate-code","46":"duplicate-code","47":"duplicate-code","48":"duplicate-code","49":"duplicate-code","50":"duplicate-code","51":"duplicate-code","52":"duplicate-code"},"text":{"0":"Consider merging these comparisons with \"in\" to \"token in ('[CLS]', '[SEP]')\"","1":"Too many branches (15\/12)","2":"Consider using enumerate instead of iterating with range and len","3":"Consider merging these comparisons with \"in\" to \"char in (' ', '\\\\t', '\\\\n', '\\\\r')\"","4":"Consider merging these comparisons with \"in\" to \"char in ('\\\\t', '\\\\n', '\\\\r')\"","5":"Similar lines in 2 files\n==run_classifier:520\n==run_squad:700\n  def _decode_record(record, name_to_features):\n    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n      t = example[name]\n      if t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n\n  def input_fn(params):\n    \"\"\"The actual input function.\"\"\"\n    batch_size = params[\"batch_size\"]\n\n    # For training, we want a lot of parallel reading and shuffling.\n    # For eval, we want no shuffling and parallel reading doesn't matter.\n    d = tf.data.TFRecordDataset(input_file)\n    if is_training:\n      d = d.repeat()\n      d = d.shuffle(buffer_size=100)\n\n    d = d.apply(\n        tf.contrib.data.map_and_batch(\n            lambda record: _decode_record(record, name_to_features),\n            batch_size=batch_size,\n            drop_remainder=drop_remainder))\n\n    return d\n\n  return input_fn\n\n","6":"Similar lines in 2 files\n==run_pretraining:149\n==run_squad:616\n    tvars = tf.trainable_variables()\n\n    initialized_variable_names = {}\n    scaffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\"**** Trainable Variables ****\")\n    for var in tvars:\n      init_string = \"\"\n      if var.name in initialized_variable_names:\n        init_string = \", *INIT_FROM_CKPT*\"\n      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:","7":"Similar lines in 2 files\n==run_classifier:99\n==run_pretraining:81\nflags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU\/CPU.\")\n\ntf.flags.DEFINE_string(\n    \"tpu_name\", None,\n    \"The Cloud TPU to use for training. This should be either the name \"\n    \"used when creating the Cloud TPU, or a grpc:\/\/ip.address.of.tpu:8470 \"\n    \"url.\")\n\ntf.flags.DEFINE_string(\n    \"tpu_zone\", None,\n    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n    \"specified, we will attempt to automatically detect the GCE project from \"\n    \"metadata.\")\n\ntf.flags.DEFINE_string(\n    \"gcp_project\", None,\n    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n    \"specified, we will attempt to automatically detect the GCE project from \"\n    \"metadata.\")\n\ntf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n\nflags.DEFINE_integer(\n    \"num_tpu_cores\", 8,\n    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n\n","8":"Similar lines in 2 files\n==run_classifier:591\n==run_classifier_with_tfhub:59\n  hidden_size = output_layer.shape[-1].value\n\n  output_weights = tf.get_variable(\n      \"output_weights\", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.get_variable(\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n\n  with tf.variable_scope(\"loss\"):\n    if is_training:\n      # I.e., 0.1 dropout\n      output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    probabilities = tf.nn.softmax(logits, axis=-1)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n\n    return (loss, per_example_loss, logits, probabilities)\n\n","9":"Similar lines in 3 files\n==run_classifier:99\n==run_pretraining:81\n==run_squad:116\nflags.DEFINE_bool(\"use_tpu\", False, \"Whether to use TPU or GPU\/CPU.\")\n\ntf.flags.DEFINE_string(\n    \"tpu_name\", None,\n    \"The Cloud TPU to use for training. This should be either the name \"\n    \"used when creating the Cloud TPU, or a grpc:\/\/ip.address.of.tpu:8470 \"\n    \"url.\")\n\ntf.flags.DEFINE_string(\n    \"tpu_zone\", None,\n    \"[Optional] GCE zone where the Cloud TPU is located in. If not \"\n    \"specified, we will attempt to automatically detect the GCE project from \"\n    \"metadata.\")\n\ntf.flags.DEFINE_string(\n    \"gcp_project\", None,\n    \"[Optional] Project name for the Cloud TPU-enabled project. If not \"\n    \"specified, we will attempt to automatically detect the GCE project from \"\n    \"metadata.\")\n\ntf.flags.DEFINE_string(\"master\", None, \"[Optional] TensorFlow master URL.\")\n\nflags.DEFINE_integer(\n    \"num_tpu_cores\", 8,\n    \"Only used if `use_tpu` is True. Total number of TPU cores to use.\")\n","10":"Similar lines in 2 files\n==run_classifier:821\n==run_classifier_with_tfhub:182\n  tpu_cluster_resolver = None\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.output_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_tpu_cores,\n          per_host_input_for_training=is_per_host))\n\n  train_examples = None\n  num_train_steps = None\n  num_warmup_steps = None\n  if FLAGS.do_train:\n    train_examples = processor.get_train_examples(FLAGS.data_dir)\n    num_train_steps = int(\n        len(train_examples) \/ FLAGS.train_batch_size * FLAGS.num_train_epochs)\n    num_warmup_steps = int(num_train_steps * FLAGS.warmup_proportion)\n\n  model_fn = model_fn_builder(","11":"Similar lines in 3 files\n==run_classifier:647\n==run_pretraining:151\n==run_squad:618\n    initialized_variable_names = {}\n    scaffold_fn = None\n    if init_checkpoint:\n      (assignment_map, initialized_variable_names\n      ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\"**** Trainable Variables ****\")\n    for var in tvars:\n      init_string = \"\"\n      if var.name in initialized_variable_names:\n        init_string = \", *INIT_FROM_CKPT*\"\n      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n                      init_string)\n\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:","12":"Similar lines in 2 files\n==run_classifier:818\n==run_squad:1134\n  tokenizer = tokenization.FullTokenizer(\n      vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)\n\n  tpu_cluster_resolver = None\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.output_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_tpu_cores,\n          per_host_input_for_training=is_per_host))\n\n  train_examples = None\n  num_train_steps = None\n  num_warmup_steps = None\n  if FLAGS.do_train:","13":"Similar lines in 2 files\n==run_classifier_with_tfhub:182\n==run_squad:1137\n  tpu_cluster_resolver = None\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.output_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_tpu_cores,\n          per_host_input_for_training=is_per_host))\n\n  train_examples = None\n  num_train_steps = None\n  num_warmup_steps = None\n  if FLAGS.do_train:","14":"Similar lines in 2 files\n==run_classifier:438\n==run_squad:386\n        segment_ids.append(1)\n      tokens.append(\"[SEP]\")\n      segment_ids.append(1)\n\n      input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n      # The mask has 1 for real tokens and 0 for padding tokens. Only real\n      # tokens are attended to.\n      input_mask = [1] * len(input_ids)\n\n      # Zero-pad up to the sequence length.\n      while len(input_ids) < max_seq_length:\n        input_ids.append(0)\n        input_mask.append(0)\n        segment_ids.append(0)\n\n      assert len(input_ids) == max_seq_length\n      assert len(input_mask) == max_seq_length\n      assert len(segment_ids) == max_seq_length\n","15":"Similar lines in 4 files\n==extract_features:175\n==run_classifier:652\n==run_pretraining:156\n==run_squad:623\n      if use_tpu:\n\n        def tpu_scaffold():\n          tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n          return tf.train.Scaffold()\n\n        scaffold_fn = tpu_scaffold\n      else:\n        tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n\n    tf.logging.info(\"**** Trainable Variables ****\")\n    for var in tvars:\n      init_string = \"\"\n      if var.name in initialized_variable_names:\n        init_string = \", *INIT_FROM_CKPT*\"\n      tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,\n                      init_string)\n","16":"Similar lines in 2 files\n==extract_features:301\n==run_classifier:556\ndef _truncate_seq_pair(tokens_a, tokens_b, max_length):\n  \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n\n  # This is a simple heuristic which will always truncate the longer sequence\n  # one token at a time. This makes more sense than truncating an equal percent\n  # of tokens from each, since if one sequence is very short then each token\n  # that's truncated likely contains more information than a longer sequence.\n  while True:\n    total_length = len(tokens_a) + len(tokens_b)\n    if total_length <= max_length:\n      break\n    if len(tokens_a) > len(tokens_b):\n      tokens_a.pop()\n    else:\n      tokens_b.pop()\n\n","17":"Similar lines in 2 files\n==run_classifier:83\n==run_squad:90\nflags.DEFINE_float(\"learning_rate\", 5e-5, \"The initial learning rate for Adam.\")\n\nflags.DEFINE_float(\"num_train_epochs\", 3.0,\n                   \"Total number of training epochs to perform.\")\n\nflags.DEFINE_float(\n    \"warmup_proportion\", 0.1,\n    \"Proportion of training to perform linear learning rate warmup for. \"\n    \"E.g., 0.1 = 10% of training.\")\n\nflags.DEFINE_integer(\"save_checkpoints_steps\", 1000,\n                     \"How often to save the model checkpoint.\")\n\nflags.DEFINE_integer(\"iterations_per_loop\", 1000,\n                     \"How many steps to make in each estimator call.\")\n","18":"Similar lines in 4 files\n==run_classifier:821\n==run_classifier_with_tfhub:182\n==run_pretraining:423\n==run_squad:1137\n  tpu_cluster_resolver = None\n  if FLAGS.use_tpu and FLAGS.tpu_name:\n    tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=FLAGS.tpu_zone, project=FLAGS.gcp_project)\n\n  is_per_host = tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2\n  run_config = tf.contrib.tpu.RunConfig(\n      cluster=tpu_cluster_resolver,\n      master=FLAGS.master,\n      model_dir=FLAGS.output_dir,\n      save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n      tpu_config=tf.contrib.tpu.TPUConfig(\n          iterations_per_loop=FLAGS.iterations_per_loop,\n          num_shards=FLAGS.num_tpu_cores,\n          per_host_input_for_training=is_per_host))\n","19":"Similar lines in 2 files\n==run_classifier:914\n==run_classifier_with_tfhub:260\n        seq_length=FLAGS.max_seq_length,\n        is_training=False,\n        drop_remainder=eval_drop_remainder)\n\n    result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)\n\n    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n      tf.logging.info(\"***** Eval results *****\")\n      for key in sorted(result.keys()):\n        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n  if FLAGS.do_predict:\n    predict_examples = processor.get_test_examples(FLAGS.data_dir)","20":"Similar lines in 2 files\n==run_classifier:849\n==run_squad:1170\n      init_checkpoint=FLAGS.init_checkpoint,\n      learning_rate=FLAGS.learning_rate,\n      num_train_steps=num_train_steps,\n      num_warmup_steps=num_warmup_steps,\n      use_tpu=FLAGS.use_tpu,\n      use_one_hot_embeddings=FLAGS.use_tpu)\n\n  # If TPU is not available, this will fall back to normal Estimator on CPU\n  # or GPU.\n  estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,","21":"Similar lines in 2 files\n==run_classifier:619\n==run_pretraining:109\n                     num_train_steps, num_warmup_steps, use_tpu,\n                     use_one_hot_embeddings):\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    tf.logging.info(\"*** Features ***\")\n    for name in sorted(features.keys()):\n      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n\n    input_ids = features[\"input_ids\"]\n    input_mask = features[\"input_mask\"]\n    segment_ids = features[\"segment_ids\"]","22":"Similar lines in 3 files\n==run_classifier:520\n==run_pretraining:390\n==run_squad:700\n  def _decode_record(record, name_to_features):\n    \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n    example = tf.parse_single_example(record, name_to_features)\n\n    # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n    # So cast all int64 to int32.\n    for name in list(example.keys()):\n      t = example[name]\n      if t.dtype == tf.int64:\n        t = tf.to_int32(t)\n      example[name] = t\n\n    return example\n","23":"Similar lines in 2 files\n==run_pretraining:108\n==run_squad:589\ndef model_fn_builder(bert_config, init_checkpoint, learning_rate,\n                     num_train_steps, num_warmup_steps, use_tpu,\n                     use_one_hot_embeddings):\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    tf.logging.info(\"*** Features ***\")\n    for name in sorted(features.keys()):\n      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n","24":"Similar lines in 2 files\n==run_pretraining:23\n==run_squad:29\nimport tensorflow as tf\n\nflags = tf.flags\n\nFLAGS = flags.FLAGS\n\n## Required parameters\nflags.DEFINE_string(\n    \"bert_config_file\", None,\n    \"The config json file corresponding to the pre-trained BERT model. \"\n    \"This specifies the model architecture.\")\n","25":"Similar lines in 2 files\n==run_classifier:619\n==run_squad:590\n                     num_train_steps, num_warmup_steps, use_tpu,\n                     use_one_hot_embeddings):\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    tf.logging.info(\"*** Features ***\")\n    for name in sorted(features.keys()):\n      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n","26":"Similar lines in 2 files\n==run_classifier:807\n==run_classifier_with_tfhub:169\n  tf.gfile.MakeDirs(FLAGS.output_dir)\n\n  task_name = FLAGS.task_name.lower()\n\n  if task_name not in processors:\n    raise ValueError(\"Task not found: %s\" % (task_name))\n\n  processor = processors[task_name]()\n\n  label_list = processor.get_labels()\n","27":"Similar lines in 2 files\n==run_classifier:623\n==run_classifier_with_tfhub:90\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    tf.logging.info(\"*** Features ***\")\n    for name in sorted(features.keys()):\n      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n\n    input_ids = features[\"input_ids\"]\n    input_mask = features[\"input_mask\"]\n    segment_ids = features[\"segment_ids\"]\n    label_ids = features[\"label_ids\"]","28":"Similar lines in 2 files\n==run_classifier_with_tfhub:90\n==run_pretraining:113\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    tf.logging.info(\"*** Features ***\")\n    for name in sorted(features.keys()):\n      tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n\n    input_ids = features[\"input_ids\"]\n    input_mask = features[\"input_mask\"]\n    segment_ids = features[\"segment_ids\"]","29":"Similar lines in 2 files\n==run_classifier:54\n==run_squad:56\nflags.DEFINE_string(\n    \"init_checkpoint\", None,\n    \"Initial checkpoint (usually from a pre-trained BERT model).\")\n\nflags.DEFINE_bool(\n    \"do_lower_case\", True,\n    \"Whether to lower case the input text. Should be True for uncased \"\n    \"models and False for cased models.\")\n\nflags.DEFINE_integer(","30":"Similar lines in 3 files\n==run_classifier:853\n==run_pretraining:445\n==run_squad:1174\n      use_tpu=FLAGS.use_tpu,\n      use_one_hot_embeddings=FLAGS.use_tpu)\n\n  # If TPU is not available, this will fall back to normal Estimator on CPU\n  # or GPU.\n  estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,","31":"Similar lines in 2 files\n==run_classifier:673\n==run_pretraining:176\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)\n    elif mode == tf.estimator.ModeKeys.EVAL:\n","32":"Similar lines in 2 files\n==run_classifier:858\n==run_classifier_with_tfhub:217\n  estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,\n      eval_batch_size=FLAGS.eval_batch_size,\n      predict_batch_size=FLAGS.predict_batch_size)\n\n  if FLAGS.do_train:","33":"Similar lines in 2 files\n==extract_features:125\n==run_classifier:736\n        \"input_ids\":\n            tf.constant(\n                all_input_ids, shape=[num_examples, seq_length],\n                dtype=tf.int32),\n        \"input_mask\":\n            tf.constant(\n                all_input_mask,\n                shape=[num_examples, seq_length],\n                dtype=tf.int32),","34":"Similar lines in 3 files\n==create_pretraining_data:111\n==run_classifier:449\n==run_squad:397\n    while len(input_ids) < max_seq_length:\n      input_ids.append(0)\n      input_mask.append(0)\n      segment_ids.append(0)\n\n    assert len(input_ids) == max_seq_length\n    assert len(input_mask) == max_seq_length\n    assert len(segment_ids) == max_seq_length\n","35":"Similar lines in 2 files\n==run_classifier_with_tfhub:108\n==run_pretraining:174\n    output_spec = None\n    if mode == tf.estimator.ModeKeys.TRAIN:\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,","36":"Similar lines in 3 files\n==run_classifier:673\n==run_pretraining:176\n==run_squad:661\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          train_op=train_op,\n          scaffold_fn=scaffold_fn)","37":"Similar lines in 2 files\n==run_classifier:45\n==run_squad:41\nflags.DEFINE_string(\"vocab_file\", None,\n                    \"The vocabulary file that the BERT model was trained on.\")\n\nflags.DEFINE_string(\n    \"output_dir\", None,\n    \"The output directory where the model checkpoints will be written.\")\n\n## Other parameters","38":"Similar lines in 3 files\n==run_classifier:576\n==run_pretraining:130\n==run_squad:552\n    model = modeling.BertModel(\n        config=bert_config,\n        is_training=is_training,\n        input_ids=input_ids,\n        input_mask=input_mask,\n        token_type_ids=segment_ids,\n        use_one_hot_embeddings=use_one_hot_embeddings)\n","39":"Similar lines in 2 files\n==create_pretraining_data:36\n==extract_features:54\nflags.DEFINE_string(\"vocab_file\", None,\n                    \"The vocabulary file that the BERT model was trained on.\")\n\nflags.DEFINE_bool(\n    \"do_lower_case\", True,\n    \"Whether to lower case the input text. Should be True for uncased \"\n    \"models and False for cased models.\")\n","40":"Similar lines in 3 files\n==run_classifier:920\n==run_classifier_with_tfhub:266\n==run_pretraining:480\n    output_eval_file = os.path.join(FLAGS.output_dir, \"eval_results.txt\")\n    with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n      tf.logging.info(\"***** Eval results *****\")\n      for key in sorted(result.keys()):\n        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","41":"Similar lines in 2 files\n==run_classifier:876\n==run_classifier_with_tfhub:234\n        seq_length=FLAGS.max_seq_length,\n        is_training=True,\n        drop_remainder=True)\n    estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n\n  if FLAGS.do_eval:\n    eval_examples = processor.get_dev_examples(FLAGS.data_dir)","42":"Similar lines in 2 files\n==create_pretraining_data:303\n==run_classifier:425\n  tokens = []\n  segment_ids = []\n  tokens.append(\"[CLS]\")\n  segment_ids.append(0)\n  for token in tokens_a:\n    tokens.append(token)\n    segment_ids.append(0)","43":"Similar lines in 2 files\n==run_classifier:801\n==run_squad:1113\n  if FLAGS.max_seq_length > bert_config.max_position_embeddings:\n    raise ValueError(\n        \"Cannot use sequence length %d because the BERT model \"\n        \"was only trained up to sequence length %d\" %\n        (FLAGS.max_seq_length, bert_config.max_position_embeddings))\n","44":"Similar lines in 2 files\n==run_classifier:695\n==run_pretraining:226\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,\n          eval_metrics=eval_metrics,\n          scaffold_fn=scaffold_fn)\n    else:","45":"Similar lines in 3 files\n==run_classifier:673\n==run_classifier_with_tfhub:110\n==run_squad:661\n      train_op = optimization.create_optimizer(\n          total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n\n      output_spec = tf.contrib.tpu.TPUEstimatorSpec(\n          mode=mode,\n          loss=total_loss,","46":"Similar lines in 2 files\n==modeling_test:14\n==run_squad:16\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\nimport json","47":"Similar lines in 2 files\n==extract_features:44\n==run_classifier:63\nflags.DEFINE_integer(\n    \"max_seq_length\", 128,\n    \"The maximum total input sequence length after WordPiece tokenization. \"\n    \"Sequences longer than this will be truncated, and sequences shorter \"\n    \"than this will be padded.\")\n","48":"Similar lines in 2 files\n==create_pretraining_data:314\n==run_classifier:436\n        for token in tokens_b:\n          tokens.append(token)\n          segment_ids.append(1)\n        tokens.append(\"[SEP]\")\n        segment_ids.append(1)\n","49":"Similar lines in 2 files\n==create_pretraining_data:191\n==extract_features:322\n  with tf.gfile.GFile(input_file, \"r\") as reader:\n    while True:\n      line = tokenization.convert_to_unicode(reader.readline())\n      if not line:\n        break\n      line = line.strip()","50":"Similar lines in 3 files\n==run_classifier_with_tfhub:217\n==run_pretraining:450\n==run_squad:1179\n  estimator = tf.contrib.tpu.TPUEstimator(\n      use_tpu=FLAGS.use_tpu,\n      model_fn=model_fn,\n      config=run_config,\n      train_batch_size=FLAGS.train_batch_size,","51":"Similar lines in 2 files\n==modeling:75\n==modeling_test:63\n      self.hidden_dropout_prob = hidden_dropout_prob\n      self.attention_probs_dropout_prob = attention_probs_dropout_prob\n      self.max_position_embeddings = max_position_embeddings\n      self.type_vocab_size = type_vocab_size\n      self.initializer_range = initializer_range","52":"Similar lines in 2 files\n==modeling:39\n==modeling_test:44\n                 hidden_act=\"gelu\",\n                 hidden_dropout_prob=0.1,\n                 attention_probs_dropout_prob=0.1,\n                 max_position_embeddings=512,\n                 type_vocab_size=16,"},"number":{"0":"R1714","1":"R0912","2":"C0200","3":"R1714","4":"R1714","5":"R0801","6":"R0801","7":"R0801","8":"R0801","9":"R0801","10":"R0801","11":"R0801","12":"R0801","13":"R0801","14":"R0801","15":"R0801","16":"R0801","17":"R0801","18":"R0801","19":"R0801","20":"R0801","21":"R0801","22":"R0801","23":"R0801","24":"R0801","25":"R0801","26":"R0801","27":"R0801","28":"R0801","29":"R0801","30":"R0801","31":"R0801","32":"R0801","33":"R0801","34":"R0801","35":"R0801","36":"R0801","37":"R0801","38":"R0801","39":"R0801","40":"R0801","41":"R0801","42":"R0801","43":"R0801","44":"R0801","45":"R0801","46":"R0801","47":"R0801","48":"R0801","49":"R0801","50":"R0801","51":"R0801","52":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint","15":"pylint","16":"pylint","17":"pylint","18":"pylint","19":"pylint","20":"pylint","21":"pylint","22":"pylint","23":"pylint","24":"pylint","25":"pylint","26":"pylint","27":"pylint","28":"pylint","29":"pylint","30":"pylint","31":"pylint","32":"pylint","33":"pylint","34":"pylint","35":"pylint","36":"pylint","37":"pylint","38":"pylint","39":"pylint","40":"pylint","41":"pylint","42":"pylint","43":"pylint","44":"pylint","45":"pylint","46":"pylint","47":"pylint","48":"pylint","49":"pylint","50":"pylint","51":"pylint","52":"pylint"},"lines_amount":{"0":470,"1":278,"2":1284,"3":400,"4":400,"5":138,"6":138,"7":138,"8":138,"9":138,"10":138,"11":138,"12":138,"13":138,"14":138,"15":138,"16":138,"17":138,"18":138,"19":138,"20":138,"21":138,"22":138,"23":138,"24":138,"25":138,"26":138,"27":138,"28":138,"29":138,"30":138,"31":138,"32":138,"33":138,"34":138,"35":138,"36":138,"37":138,"38":138,"39":138,"40":138,"41":138,"42":138,"43":138,"44":138,"45":138,"46":138,"47":138,"48":138,"49":138,"50":138,"51":138,"52":138},"commit":{"0":"eedf5716ce1268e56f0a50264a88cafad334ac61","1":"eedf5716ce1268e56f0a50264a88cafad334ac61","2":"eedf5716ce1268e56f0a50264a88cafad334ac61","3":"eedf5716ce1268e56f0a50264a88cafad334ac61","4":"eedf5716ce1268e56f0a50264a88cafad334ac61","5":"eedf5716ce1268e56f0a50264a88cafad334ac61","6":"eedf5716ce1268e56f0a50264a88cafad334ac61","7":"eedf5716ce1268e56f0a50264a88cafad334ac61","8":"eedf5716ce1268e56f0a50264a88cafad334ac61","9":"eedf5716ce1268e56f0a50264a88cafad334ac61","10":"eedf5716ce1268e56f0a50264a88cafad334ac61","11":"eedf5716ce1268e56f0a50264a88cafad334ac61","12":"eedf5716ce1268e56f0a50264a88cafad334ac61","13":"eedf5716ce1268e56f0a50264a88cafad334ac61","14":"eedf5716ce1268e56f0a50264a88cafad334ac61","15":"eedf5716ce1268e56f0a50264a88cafad334ac61","16":"eedf5716ce1268e56f0a50264a88cafad334ac61","17":"eedf5716ce1268e56f0a50264a88cafad334ac61","18":"eedf5716ce1268e56f0a50264a88cafad334ac61","19":"eedf5716ce1268e56f0a50264a88cafad334ac61","20":"eedf5716ce1268e56f0a50264a88cafad334ac61","21":"eedf5716ce1268e56f0a50264a88cafad334ac61","22":"eedf5716ce1268e56f0a50264a88cafad334ac61","23":"eedf5716ce1268e56f0a50264a88cafad334ac61","24":"eedf5716ce1268e56f0a50264a88cafad334ac61","25":"eedf5716ce1268e56f0a50264a88cafad334ac61","26":"eedf5716ce1268e56f0a50264a88cafad334ac61","27":"eedf5716ce1268e56f0a50264a88cafad334ac61","28":"eedf5716ce1268e56f0a50264a88cafad334ac61","29":"eedf5716ce1268e56f0a50264a88cafad334ac61","30":"eedf5716ce1268e56f0a50264a88cafad334ac61","31":"eedf5716ce1268e56f0a50264a88cafad334ac61","32":"eedf5716ce1268e56f0a50264a88cafad334ac61","33":"eedf5716ce1268e56f0a50264a88cafad334ac61","34":"eedf5716ce1268e56f0a50264a88cafad334ac61","35":"eedf5716ce1268e56f0a50264a88cafad334ac61","36":"eedf5716ce1268e56f0a50264a88cafad334ac61","37":"eedf5716ce1268e56f0a50264a88cafad334ac61","38":"eedf5716ce1268e56f0a50264a88cafad334ac61","39":"eedf5716ce1268e56f0a50264a88cafad334ac61","40":"eedf5716ce1268e56f0a50264a88cafad334ac61","41":"eedf5716ce1268e56f0a50264a88cafad334ac61","42":"eedf5716ce1268e56f0a50264a88cafad334ac61","43":"eedf5716ce1268e56f0a50264a88cafad334ac61","44":"eedf5716ce1268e56f0a50264a88cafad334ac61","45":"eedf5716ce1268e56f0a50264a88cafad334ac61","46":"eedf5716ce1268e56f0a50264a88cafad334ac61","47":"eedf5716ce1268e56f0a50264a88cafad334ac61","48":"eedf5716ce1268e56f0a50264a88cafad334ac61","49":"eedf5716ce1268e56f0a50264a88cafad334ac61","50":"eedf5716ce1268e56f0a50264a88cafad334ac61","51":"eedf5716ce1268e56f0a50264a88cafad334ac61","52":"eedf5716ce1268e56f0a50264a88cafad334ac61"},"repo":{"0":"google-research\/bert","1":"google-research\/bert","2":"google-research\/bert","3":"google-research\/bert","4":"google-research\/bert","5":"google-research\/bert","6":"google-research\/bert","7":"google-research\/bert","8":"google-research\/bert","9":"google-research\/bert","10":"google-research\/bert","11":"google-research\/bert","12":"google-research\/bert","13":"google-research\/bert","14":"google-research\/bert","15":"google-research\/bert","16":"google-research\/bert","17":"google-research\/bert","18":"google-research\/bert","19":"google-research\/bert","20":"google-research\/bert","21":"google-research\/bert","22":"google-research\/bert","23":"google-research\/bert","24":"google-research\/bert","25":"google-research\/bert","26":"google-research\/bert","27":"google-research\/bert","28":"google-research\/bert","29":"google-research\/bert","30":"google-research\/bert","31":"google-research\/bert","32":"google-research\/bert","33":"google-research\/bert","34":"google-research\/bert","35":"google-research\/bert","36":"google-research\/bert","37":"google-research\/bert","38":"google-research\/bert","39":"google-research\/bert","40":"google-research\/bert","41":"google-research\/bert","42":"google-research\/bert","43":"google-research\/bert","44":"google-research\/bert","45":"google-research\/bert","46":"google-research\/bert","47":"google-research\/bert","48":"google-research\/bert","49":"google-research\/bert","50":"google-research\/bert","51":"google-research\/bert","52":"google-research\/bert"}}