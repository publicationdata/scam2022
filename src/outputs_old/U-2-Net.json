{"type":{"0":"C","1":"W","2":"C","3":"R","4":"R","5":"R","6":"R","7":"R","8":"R","9":"R","10":"R","11":"R","12":"R","13":"R","14":"R"},"module":{"0":"u2net_portrait_demo","1":"u2net_portrait_demo","2":"u2net_portrait_demo","3":"u2net_train","4":"u2net_train","5":"u2net_train","6":"u2net_train","7":"u2net_train","8":"u2net_train","9":"u2net_train","10":"u2net_train","11":"u2net_train","12":"u2net_train","13":"u2net_train","14":"u2net_train"},"obj":{"0":"detect_single_face","1":"inference","2":"main","3":"","4":"","5":"","6":"","7":"","8":"","9":"","10":"","11":"","12":"","13":"","14":""},"lnum":{"0":22,"1":99,"2":160,"3":1,"4":1,"5":1,"6":1,"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1},"col":{"0":4,"1":18,"2":4,"3":0,"4":0,"5":0,"6":0,"7":0,"8":0,"9":0,"10":0,"11":0,"12":0,"13":0,"14":0},"filename":{"0":"u2net_portrait_demo.py","1":"u2net_portrait_demo.py","2":"u2net_portrait_demo.py","3":"u2net_train.py","4":"u2net_train.py","5":"u2net_train.py","6":"u2net_train.py","7":"u2net_train.py","8":"u2net_train.py","9":"u2net_train.py","10":"u2net_train.py","11":"u2net_train.py","12":"u2net_train.py","13":"u2net_train.py","14":"u2net_train.py"},"symbol":{"0":"consider-using-enumerate","1":"redefined-builtin","2":"consider-using-enumerate","3":"duplicate-code","4":"duplicate-code","5":"duplicate-code","6":"duplicate-code","7":"duplicate-code","8":"duplicate-code","9":"duplicate-code","10":"duplicate-code","11":"duplicate-code","12":"duplicate-code","13":"duplicate-code","14":"duplicate-code"},"text":{"0":"Consider using enumerate instead of iterating with range and len","1":"Redefining built-in 'input'","2":"Consider using enumerate instead of iterating with range and len","3":"Similar lines in 2 files\n==u2net_portrait_test:0\n==u2net_test:0\nimport os\nfrom skimage import io, transform\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms#, utils\n# import torch.optim as optim\n\nimport numpy as np\nfrom PIL import Image\nimport glob\n\nfrom data_loader import RescaleT\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB\nfrom model import U2NETP # small version u2net 4.7 MB\n\n# normalize the predicted SOD probability map\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)\/(ma-mi)\n\n    return dn\n\ndef save_output(image_name,pred,d_dir):\n\n    predict = pred\n    predict = predict.squeeze()\n    predict_np = predict.cpu().data.numpy()\n\n    im = Image.fromarray(predict_np*255).convert('RGB')\n    img_name = image_name.split(os.sep)[-1]\n    image = io.imread(image_name)\n    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n\n    pb_np = np.array(imo)\n\n    aaa = img_name.split(\".\")\n    bbb = aaa[0:-1]\n    imidx = bbb[0]\n    for i in range(1,len(bbb)):\n        imidx = imidx + \".\" + bbb[i]\n","4":"Similar lines in 2 files\n==u2net_human_seg_test:82\n==u2net_test:87\n    if torch.cuda.is_available():\n        net.load_state_dict(torch.load(model_dir))\n        net.cuda()\n    else:\n        net.load_state_dict(torch.load(model_dir, map_location='cpu'))\n    net.eval()\n\n    # --------- 4. inference for each image ---------\n    for i_test, data_test in enumerate(test_salobj_dataloader):\n\n        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n\n        inputs_test = data_test['image']\n        inputs_test = inputs_test.type(torch.FloatTensor)\n\n        if torch.cuda.is_available():\n            inputs_test = Variable(inputs_test.cuda())\n        else:\n            inputs_test = Variable(inputs_test)\n\n        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n\n        # normalization\n        pred = d1[:,0,:,:]\n        pred = normPRED(pred)\n\n        # save results to test_results folder\n        if not os.path.exists(prediction_dir):\n            os.makedirs(prediction_dir, exist_ok=True)\n        save_output(img_name_list[i_test],pred,prediction_dir)\n\n        del d1,d2,d3,d4,d5,d6,d7\n\nif __name__ == \"__main__\":\n    main()","5":"Similar lines in 2 files\n==u2net_human_seg_test:23\n==u2net_test:24\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)\/(ma-mi)\n\n    return dn\n\ndef save_output(image_name,pred,d_dir):\n\n    predict = pred\n    predict = predict.squeeze()\n    predict_np = predict.cpu().data.numpy()\n\n    im = Image.fromarray(predict_np*255).convert('RGB')\n    img_name = image_name.split(os.sep)[-1]\n    image = io.imread(image_name)\n    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n\n    pb_np = np.array(imo)\n\n    aaa = img_name.split(\".\")\n    bbb = aaa[0:-1]\n    imidx = bbb[0]\n    for i in range(1,len(bbb)):\n        imidx = imidx + \".\" + bbb[i]\n\n    imo.save(d_dir+imidx+'.png')\n\ndef main():\n\n    # --------- 1. get image path and name ---------\n    model_name='u2net'#u2netp\n\n","6":"Similar lines in 2 files\n==u2net_human_seg_test:23\n==u2net_portrait_test:24\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)\/(ma-mi)\n\n    return dn\n\ndef save_output(image_name,pred,d_dir):\n\n    predict = pred\n    predict = predict.squeeze()\n    predict_np = predict.cpu().data.numpy()\n\n    im = Image.fromarray(predict_np*255).convert('RGB')\n    img_name = image_name.split(os.sep)[-1]\n    image = io.imread(image_name)\n    imo = im.resize((image.shape[1],image.shape[0]),resample=Image.BILINEAR)\n\n    pb_np = np.array(imo)\n\n    aaa = img_name.split(\".\")\n    bbb = aaa[0:-1]\n    imidx = bbb[0]\n    for i in range(1,len(bbb)):\n        imidx = imidx + \".\" + bbb[i]\n","7":"Similar lines in 3 files\n==u2net_human_seg_test:0\n==u2net_portrait_test:0\n==u2net_test:0\nimport os\nfrom skimage import io, transform\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms#, utils\n# import torch.optim as optim\n\nimport numpy as np\nfrom PIL import Image\nimport glob\n\nfrom data_loader import RescaleT\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB","8":"Similar lines in 2 files\n==u2net_human_seg_test:62\n==u2net_test:64\n    img_name_list = glob.glob(image_dir + os.sep + '*')\n    print(img_name_list)\n\n    # --------- 2. dataloader ---------\n    #1. dataloader\n    test_salobj_dataset = SalObjDataset(img_name_list = img_name_list,\n                                        lbl_name_list = [],\n                                        transform=transforms.Compose([RescaleT(320),\n                                                                      ToTensorLab(flag=0)])\n                                        )\n    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n                                        batch_size=1,\n                                        shuffle=False,\n                                        num_workers=1)\n\n    # --------- 3. model define ---------\n    if(model_name=='u2net'):\n        print(\"...load U2NET---173.6 MB\")\n        net = U2NET(3,1)","9":"Similar lines in 3 files\n==u2net_human_seg_test:87\n==u2net_portrait_test:89\n==u2net_test:92\n    net.eval()\n\n    # --------- 4. inference for each image ---------\n    for i_test, data_test in enumerate(test_salobj_dataloader):\n\n        print(\"inferencing:\",img_name_list[i_test].split(os.sep)[-1])\n\n        inputs_test = data_test['image']\n        inputs_test = inputs_test.type(torch.FloatTensor)\n\n        if torch.cuda.is_available():\n            inputs_test = Variable(inputs_test.cuda())\n        else:\n            inputs_test = Variable(inputs_test)\n\n        d1,d2,d3,d4,d5,d6,d7= net(inputs_test)\n\n        # normalization","10":"Similar lines in 3 files\n==u2net_portrait_test:16\n==u2net_test:16\n==u2net_train:19\nfrom data_loader import ToTensor\nfrom data_loader import ToTensorLab\nfrom data_loader import SalObjDataset\n\nfrom model import U2NET # full size version 173.6 MB\nfrom model import U2NETP # small version u2net 4.7 MB\n\n# normalize the predicted SOD probability map","11":"Similar lines in 3 files\n==u2net_human_seg_test:70\n==u2net_portrait_test:74\n==u2net_test:72\n                                                                      ToTensorLab(flag=0)])\n                                        )\n    test_salobj_dataloader = DataLoader(test_salobj_dataset,\n                                        batch_size=1,\n                                        shuffle=False,\n                                        num_workers=1)\n\n    # --------- 3. model define ---------","12":"Similar lines in 4 files\n==u2net_human_seg_test:23\n==u2net_portrait_demo:90\n==u2net_portrait_test:24\n==u2net_test:24\ndef normPRED(d):\n    ma = torch.max(d)\n    mi = torch.min(d)\n\n    dn = (d-mi)\/(ma-mi)\n\n    return dn\n","13":"Similar lines in 4 files\n==u2net_human_seg_test:44\n==u2net_portrait_test:45\n==u2net_test:45\n==u2net_train:71\n    aaa = img_name.split(\".\")\n    bbb = aaa[0:-1]\n    imidx = bbb[0]\n    for i in range(1,len(bbb)):\n        imidx = imidx + \".\" + bbb[i]\n","14":"Similar lines in 4 files\n==u2net_human_seg_test:2\n==u2net_portrait_test:2\n==u2net_test:2\n==u2net_train:1\nimport torch\nimport torchvision\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F"},"number":{"0":"C0200","1":"W0622","2":"C0200","3":"R0801","4":"R0801","5":"R0801","6":"R0801","7":"R0801","8":"R0801","9":"R0801","10":"R0801","11":"R0801","12":"R0801","13":"R0801","14":"R0801"},"linter":{"0":"pylint","1":"pylint","2":"pylint","3":"pylint","4":"pylint","5":"pylint","6":"pylint","7":"pylint","8":"pylint","9":"pylint","10":"pylint","11":"pylint","12":"pylint","13":"pylint","14":"pylint"},"lines_amount":{"0":176,"1":176,"2":176,"3":165,"4":165,"5":165,"6":165,"7":165,"8":165,"9":165,"10":165,"11":165,"12":165,"13":165,"14":165},"commit":{"0":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","1":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","2":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","3":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","4":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","5":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","6":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","7":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","8":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","9":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","10":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","11":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","12":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","13":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6","14":"f2b8e4ac1c4fbe90daba8707bca051a0ec830bf6"},"repo":{"0":"xuebinqin\/U-2-Net","1":"xuebinqin\/U-2-Net","2":"xuebinqin\/U-2-Net","3":"xuebinqin\/U-2-Net","4":"xuebinqin\/U-2-Net","5":"xuebinqin\/U-2-Net","6":"xuebinqin\/U-2-Net","7":"xuebinqin\/U-2-Net","8":"xuebinqin\/U-2-Net","9":"xuebinqin\/U-2-Net","10":"xuebinqin\/U-2-Net","11":"xuebinqin\/U-2-Net","12":"xuebinqin\/U-2-Net","13":"xuebinqin\/U-2-Net","14":"xuebinqin\/U-2-Net"}}